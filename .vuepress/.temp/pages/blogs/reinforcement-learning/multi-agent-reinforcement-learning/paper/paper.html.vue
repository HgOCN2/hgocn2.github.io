<template><div><h1 id="多智能体强化学习论文" tabindex="-1"><a class="header-anchor" href="#多智能体强化学习论文" aria-hidden="true">#</a> 多智能体强化学习论文</h1>
<ul>
<li><RouterLink to="/blogs/reinforcement-learning/multi-agent-reinforcement-learning/multi-agent-reinforcement-learning.html">返回上层目录</RouterLink></li>
<li><RouterLink to="/blogs/reinforcement-learning/multi-agent-reinforcement-learning/paper/mean-field/Mean-Field-Multi-Agent-Reinforcement-Learning.html">mean-field: Mean Field Multi-Agent Reinforcement Learning ICML2018</RouterLink></li>
</ul>
<p>===</p>
<p><a href="https://zhuanlan.zhihu.com/p/638551994" target="_blank" rel="noopener noreferrer">Is Cooperative MARL Solved?<ExternalLinkIcon/></a></p>
<p>个人认为，cooperative multi-agent reinforcement learning在发现了基于策略梯度的sequential rollout机制以后就算是解决了，剩下就是对sequential rollout工程实现上的修补改进（比如引入order matters, transformer结构, distill方法等）。</p>
<p><a href="https://baijiahao.baidu.com/s?id=1734508317413537582" target="_blank" rel="noopener noreferrer">多智能体Transfermor<ExternalLinkIcon/></a></p>
<ul>
<li>论文地址：https://arxiv.org/pdf/2205.14953.pdf</li>
<li>项目主页：https://sites.google.com/view/multi-agent-transformer</li>
</ul>
<p><a href="https://zhuanlan.zhihu.com/p/635389539" target="_blank" rel="noopener noreferrer">多智能体在连续动作空间中使用Qmix方法（FACMAC）<ExternalLinkIcon/></a></p>
</div></template>


