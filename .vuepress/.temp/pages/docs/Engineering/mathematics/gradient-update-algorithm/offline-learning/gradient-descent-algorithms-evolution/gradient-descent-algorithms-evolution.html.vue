<template><div><h1 id="梯度下降算法的演化" tabindex="-1"><a class="header-anchor" href="#梯度下降算法的演化" aria-hidden="true">#</a> 梯度下降算法的演化</h1>
<ul>
<li><RouterLink to="/docs/Engineering/mathematics/gradient-update-algorithm/offline-learning/offline-learning.html">返回上层目录</RouterLink></li>
<li><a href="#%E5%90%84%E7%B1%BB%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%9A%84%E6%BC%94%E5%8C%96">各类梯度下降算法的演化</a></li>
</ul>
<p><img src="@source/docs/Engineering/mathematics/gradient-update-algorithm/offline-learning/gradient-descent-algorithms-evolution/pic/optimization-on-loss-surface-contours.gif" alt="optimization-on-loss-surface-contours"></p>
<h1 id="各类梯度下降算法的演化" tabindex="-1"><a class="header-anchor" href="#各类梯度下降算法的演化" aria-hidden="true">#</a> 各类梯度下降算法的演化</h1>
<p><img src="@source/docs/Engineering/mathematics/gradient-update-algorithm/offline-learning/gradient-descent-algorithms-evolution/pic/revolution-of-gradient-descent.jpeg" alt="revolution-of-gradient-descent"></p>
<h1 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料" aria-hidden="true">#</a> 参考资料</h1>
<ul>
<li>
<p><a href="https://blog.csdn.net/BVL10101111/article/details/72614711" target="_blank" rel="noopener noreferrer">Deep Learning 之 最优化方法<ExternalLinkIcon/></a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/32626442" target="_blank" rel="noopener noreferrer">从 SGD 到 Adam —— 深度学习优化算法概览(一)<ExternalLinkIcon/></a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/22252270" target="_blank" rel="noopener noreferrer">深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）<ExternalLinkIcon/></a></p>
</li>
<li>
<p><a href="https://ai.yanxishe.com/page/TextTranslation/1603?from=singlemessage" target="_blank" rel="noopener noreferrer">10个梯度下降优化算法+备忘单<ExternalLinkIcon/></a></p>
<p><a href="https://www.cnblogs.com/guoyaohua/p/8542554.html" target="_blank" rel="noopener noreferrer">深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）<ExternalLinkIcon/></a></p>
</li>
</ul>
</div></template>


