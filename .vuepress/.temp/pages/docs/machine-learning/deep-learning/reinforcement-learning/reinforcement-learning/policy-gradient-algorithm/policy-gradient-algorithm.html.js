export const data = JSON.parse("{\"key\":\"v-2175f9ee\",\"path\":\"/docs/machine-learning/deep-learning/reinforcement-learning/reinforcement-learning/policy-gradient-algorithm/policy-gradient-algorithm.html\",\"title\":\"策略梯度算法\",\"lang\":\"en-US\",\"frontmatter\":{},\"headers\":[{\"level\":2,\"title\":\"基于策略的强化学习\",\"slug\":\"基于策略的强化学习\",\"link\":\"#基于策略的强化学习\",\"children\":[]},{\"level\":2,\"title\":\"强化学习分类\",\"slug\":\"强化学习分类\",\"link\":\"#强化学习分类\",\"children\":[]},{\"level\":2,\"title\":\"为什么要使用策略梯度算法\",\"slug\":\"为什么要使用策略梯度算法\",\"link\":\"#为什么要使用策略梯度算法\",\"children\":[]},{\"level\":2,\"title\":\"策略模型的建模方式\",\"slug\":\"策略模型的建模方式\",\"link\":\"#策略模型的建模方式\",\"children\":[]},{\"level\":2,\"title\":\"策略梯度算法的优缺点\",\"slug\":\"策略梯度算法的优缺点\",\"link\":\"#策略梯度算法的优缺点\",\"children\":[]},{\"level\":2,\"title\":\"随机策略\",\"slug\":\"随机策略\",\"link\":\"#随机策略\",\"children\":[]},{\"level\":2,\"title\":\"策略退化\",\"slug\":\"策略退化\",\"link\":\"#策略退化\",\"children\":[]},{\"level\":2,\"title\":\"收敛性对比\",\"slug\":\"收敛性对比\",\"link\":\"#收敛性对比\",\"children\":[]},{\"level\":2,\"title\":\"策略梯度目标函数\",\"slug\":\"策略梯度目标函数\",\"link\":\"#策略梯度目标函数\",\"children\":[]},{\"level\":2,\"title\":\"数值法求梯度\",\"slug\":\"数值法求梯度\",\"link\":\"#数值法求梯度\",\"children\":[]},{\"level\":2,\"title\":\"策略梯度算法\",\"slug\":\"策略梯度算法-1\",\"link\":\"#策略梯度算法-1\",\"children\":[]},{\"level\":2,\"title\":\"策略梯度的推导\",\"slug\":\"策略梯度的推导\",\"link\":\"#策略梯度的推导\",\"children\":[]},{\"level\":2,\"title\":\"对目标函数的几点说明\",\"slug\":\"对目标函数的几点说明\",\"link\":\"#对目标函数的几点说明\",\"children\":[]},{\"level\":2,\"title\":\"求解▽θU(θ)\",\"slug\":\"求解▽θu-θ\",\"link\":\"#求解▽θu-θ\",\"children\":[]},{\"level\":2,\"title\":\"从似然率的角度\",\"slug\":\"从似然率的角度\",\"link\":\"#从似然率的角度\",\"children\":[]},{\"level\":2,\"title\":\"从重要性采样的角度\",\"slug\":\"从重要性采样的角度\",\"link\":\"#从重要性采样的角度\",\"children\":[]},{\"level\":2,\"title\":\"似然率梯度的直观理解\",\"slug\":\"似然率梯度的直观理解\",\"link\":\"#似然率梯度的直观理解\",\"children\":[]},{\"level\":2,\"title\":\"将轨迹分解成状态和动作\",\"slug\":\"将轨迹分解成状态和动作\",\"link\":\"#将轨迹分解成状态和动作\",\"children\":[]},{\"level\":2,\"title\":\"求解动作策略的梯度\",\"slug\":\"求解动作策略的梯度\",\"link\":\"#求解动作策略的梯度\",\"children\":[]},{\"level\":2,\"title\":\"似然率梯度估计\",\"slug\":\"似然率梯度估计\",\"link\":\"#似然率梯度估计\",\"children\":[]},{\"level\":2,\"title\":\"引入基线\",\"slug\":\"引入基线\",\"link\":\"#引入基线\",\"children\":[]},{\"level\":2,\"title\":\"怎么选基线\",\"slug\":\"怎么选基线\",\"link\":\"#怎么选基线\",\"children\":[]},{\"level\":2,\"title\":\"修改回报函数R(τ)\",\"slug\":\"修改回报函数r-τ\",\"link\":\"#修改回报函数r-τ\",\"children\":[]},{\"level\":2,\"title\":\"实际更新算法\",\"slug\":\"实际更新算法\",\"link\":\"#实际更新算法\",\"children\":[]},{\"level\":2,\"title\":\"蒙特卡洛策略梯度（REINFORCE）\",\"slug\":\"蒙特卡洛策略梯度-reinforce\",\"link\":\"#蒙特卡洛策略梯度-reinforce\",\"children\":[]},{\"level\":2,\"title\":\"使用Critic函数减小方差\",\"slug\":\"使用critic函数减小方差\",\"link\":\"#使用critic函数减小方差\",\"children\":[]},{\"level\":2,\"title\":\"如何理解Actor-Critic算法\",\"slug\":\"如何理解actor-critic算法\",\"link\":\"#如何理解actor-critic算法\",\"children\":[]},{\"level\":2,\"title\":\"使用优势函数减小方差\",\"slug\":\"使用优势函数减小方差\",\"link\":\"#使用优势函数减小方差\",\"children\":[]},{\"level\":2,\"title\":\"使用TD误差替代优势函数\",\"slug\":\"使用td误差替代优势函数\",\"link\":\"#使用td误差替代优势函数\",\"children\":[]},{\"level\":2,\"title\":\"带资格迹的策略梯度\",\"slug\":\"带资格迹的策略梯度\",\"link\":\"#带资格迹的策略梯度\",\"children\":[]},{\"level\":2,\"title\":\"小结\",\"slug\":\"小结\",\"link\":\"#小结\",\"children\":[]},{\"level\":2,\"title\":\"A2C\",\"slug\":\"a2c\",\"link\":\"#a2c\",\"children\":[]},{\"level\":2,\"title\":\"其他策略梯度算法\",\"slug\":\"其他策略梯度算法\",\"link\":\"#其他策略梯度算法\",\"children\":[]}],\"git\":{},\"filePathRelative\":\"docs/machine-learning/deep-learning/reinforcement-learning/reinforcement-learning/policy-gradient-algorithm/policy-gradient-algorithm.md\"}")

if (import.meta.webpackHot) {
  import.meta.webpackHot.accept()
  if (__VUE_HMR_RUNTIME__.updatePageData) {
    __VUE_HMR_RUNTIME__.updatePageData(data)
  }
}

if (import.meta.hot) {
  import.meta.hot.accept(({ data }) => {
    __VUE_HMR_RUNTIME__.updatePageData(data)
  })
}
