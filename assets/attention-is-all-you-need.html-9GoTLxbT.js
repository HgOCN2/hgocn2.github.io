const e=JSON.parse('{"key":"v-7532c661","path":"/docs/machine-learning/natural-language-processing/self-attention-and-transformer/attention-is-all-you-need/attention-is-all-you-need.html","title":"Attention Is All You Need","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"RNN的缺陷","slug":"rnn的缺陷","link":"#rnn的缺陷","children":[]},{"level":2,"title":"Transformer为何优于RNN","slug":"transformer为何优于rnn","link":"#transformer为何优于rnn","children":[]},{"level":2,"title":"Self-Attention机制","slug":"self-attention机制","link":"#self-attention机制","children":[]},{"level":2,"title":"multi-headed-Attention","slug":"multi-headed-attention","link":"#multi-headed-attention","children":[]},{"level":2,"title":"词向量Embedding输入","slug":"词向量embedding输入","link":"#词向量embedding输入","children":[]},{"level":2,"title":"位置编码","slug":"位置编码","link":"#位置编码","children":[]},{"level":2,"title":"skip-connection和Layer-Normalization","slug":"skip-connection和layer-normalization","link":"#skip-connection和layer-normalization","children":[]},{"level":2,"title":"Encoder模块汇总","slug":"encoder模块汇总","link":"#encoder模块汇总","children":[]},{"level":2,"title":"Decoder的Mask-Multi-Head-Attention输入端","slug":"decoder的mask-multi-head-attention输入端","link":"#decoder的mask-multi-head-attention输入端","children":[]},{"level":2,"title":"Decoder的Encode-Decode注意力层","slug":"decoder的encode-decode注意力层","link":"#decoder的encode-decode注意力层","children":[]},{"level":2,"title":"Decoder的输出","slug":"decoder的输出","link":"#decoder的输出","children":[]}],"git":{"createdTime":1702351416000,"updatedTime":1702351416000,"contributors":[{"name":"hsqure","email":"hgocn2@gmail.com","commits":1}]},"filePathRelative":"docs/machine-learning/natural-language-processing/self-attention-and-transformer/attention-is-all-you-need/attention-is-all-you-need.md"}');export{e as data};
