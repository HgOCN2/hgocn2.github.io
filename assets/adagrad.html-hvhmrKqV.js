import{_ as l,r as a,o as d,c as s,b as e,e as t,w as c,d as n}from"./app-VitiyI7N.js";const i={},_=e("h1",{id:"adagrad",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#adagrad","aria-hidden":"true"},"#"),n(" AdaGrad")],-1),h={href:"https://blog.csdn.net/bvl10101111/article/details/72616097",target:"_blank",rel:"noopener noreferrer"},u=e("p",null,"1.简单来讲，设置全局学习率之后，每次通过，全局学习率逐参数的除以历史梯度平方和的平方根，使得每个参数的学习率不同",-1),p=e("p",null,"2.效果是：在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小）",-1),f=e("p",null,"3.缺点是,使得学习率过早，过量的减少",-1),m=e("p",null,"4.在某些模型上效果不错。",-1);function g(x,k){const o=a("RouterLink"),r=a("ExternalLinkIcon");return d(),s("div",null,[_,e("ul",null,[e("li",null,[t(o,{to:"/docs/machine-learning/mathematics/gradient-update-algorithm/offline-learning/offline-learning.html"},{default:c(()=>[n("返回上层目录")]),_:1})])]),e("p",null,[e("a",h,[n("Deep Learning 最优化方法之AdaGrad"),t(r)])]),u,p,f,m])}const L=l(i,[["render",g],["__file","adagrad.html.vue"]]);export{L as default};
