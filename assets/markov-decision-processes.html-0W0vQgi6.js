const l=JSON.parse('{"key":"v-02612a8e","path":"/docs/machine-learning/reinforcement-learning/reinforcement-learning/markov-decision-processes/markov-decision-processes.html","title":"马尔科夫决策过程","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"马尔科夫性","slug":"马尔科夫性","link":"#马尔科夫性","children":[]},{"level":2,"title":"状态转移矩阵","slug":"状态转移矩阵","link":"#状态转移矩阵","children":[]},{"level":2,"title":"状态转移函数","slug":"状态转移函数","link":"#状态转移函数","children":[]},{"level":2,"title":"马尔科夫过程","slug":"马尔科夫过程-1","link":"#马尔科夫过程-1","children":[]},{"level":2,"title":"片段","slug":"片段","link":"#片段","children":[]},{"level":2,"title":"马尔科夫链的例子","slug":"马尔科夫链的例子","link":"#马尔科夫链的例子","children":[]},{"level":2,"title":"马尔科夫奖励过程例子","slug":"马尔科夫奖励过程例子","link":"#马尔科夫奖励过程例子","children":[]},{"level":2,"title":"回报值","slug":"回报值","link":"#回报值","children":[]},{"level":2,"title":"再聊片段","slug":"再聊片段","link":"#再聊片段","children":[]},{"level":2,"title":"再聊衰减值","slug":"再聊衰减值","link":"#再聊衰减值","children":[]},{"level":2,"title":"值函数","slug":"值函数","link":"#值函数","children":[]},{"level":2,"title":"例子：回报值和值函数","slug":"例子-回报值和值函数","link":"#例子-回报值和值函数","children":[{"level":3,"title":"回报值","slug":"回报值-1","link":"#回报值-1","children":[]},{"level":3,"title":"针对例子的V函数值","slug":"针对例子的v函数值","link":"#针对例子的v函数值","children":[]}]},{"level":2,"title":"MRPs中的贝尔曼方程","slug":"mrps中的贝尔曼方程","link":"#mrps中的贝尔曼方程","children":[{"level":3,"title":"针对例子的贝尔曼方程","slug":"针对例子的贝尔曼方程","link":"#针对例子的贝尔曼方程","children":[]},{"level":3,"title":"贝尔曼方程的矩阵形式","slug":"贝尔曼方程的矩阵形式","link":"#贝尔曼方程的矩阵形式","children":[]}]},{"level":2,"title":"马尔科夫决策过程的定义","slug":"马尔科夫决策过程的定义","link":"#马尔科夫决策过程的定义","children":[{"level":3,"title":"MDPs例子","slug":"mdps例子","link":"#mdps例子","children":[]}]},{"level":2,"title":"策略","slug":"策略","link":"#策略","children":[]},{"level":2,"title":"MDPs和MRPs之间的关系","slug":"mdps和mrps之间的关系","link":"#mdps和mrps之间的关系","children":[]},{"level":2,"title":"MDPs中的值函数","slug":"mdps中的值函数","link":"#mdps中的值函数","children":[]},{"level":2,"title":"贝尔曼期望方程","slug":"贝尔曼期望方程","link":"#贝尔曼期望方程","children":[{"level":3,"title":"V函数与Q函数之间的相互转化","slug":"v函数与q函数之间的相互转化","link":"#v函数与q函数之间的相互转化","children":[]},{"level":3,"title":"贝尔曼期望方程—V函数","slug":"贝尔曼期望方程—v函数","link":"#贝尔曼期望方程—v函数","children":[]},{"level":3,"title":"贝尔曼期望方程—Q函数","slug":"贝尔曼期望方程—q函数","link":"#贝尔曼期望方程—q函数","children":[]},{"level":3,"title":"贝尔曼期望方程例子","slug":"贝尔曼期望方程例子","link":"#贝尔曼期望方程例子","children":[]},{"level":3,"title":"贝尔曼期望方程的矩阵形式","slug":"贝尔曼期望方程的矩阵形式","link":"#贝尔曼期望方程的矩阵形式","children":[]}]},{"level":2,"title":"最优值函数","slug":"最优值函数","link":"#最优值函数","children":[{"level":3,"title":"最优V函数","slug":"最优v函数","link":"#最优v函数","children":[]},{"level":3,"title":"最优Q函数","slug":"最优q函数","link":"#最优q函数","children":[]}]},{"level":2,"title":"最优策略","slug":"最优策略","link":"#最优策略","children":[{"level":3,"title":"最优策略定理","slug":"最优策略定理","link":"#最优策略定理","children":[]},{"level":3,"title":"怎么得到最优策略","slug":"怎么得到最优策略","link":"#怎么得到最优策略","children":[]},{"level":3,"title":"最优策略例子","slug":"最优策略例子","link":"#最优策略例子","children":[]}]},{"level":2,"title":"v*与q*的相互转化","slug":"v-与q-的相互转化","link":"#v-与q-的相互转化","children":[]},{"level":2,"title":"贝尔曼最优方程","slug":"贝尔曼最优方程","link":"#贝尔曼最优方程","children":[{"level":3,"title":"最优V函数","slug":"最优v函数-1","link":"#最优v函数-1","children":[]},{"level":3,"title":"最优Q函数","slug":"最优q函数-1","link":"#最优q函数-1","children":[]},{"level":3,"title":"和贝尔曼期望方程的关系","slug":"和贝尔曼期望方程的关系","link":"#和贝尔曼期望方程的关系","children":[]},{"level":3,"title":"解贝尔曼最优方程","slug":"解贝尔曼最优方程","link":"#解贝尔曼最优方程","children":[]}]},{"level":2,"title":"无穷或连续MDPs","slug":"无穷或连续mdps","link":"#无穷或连续mdps","children":[]},{"level":2,"title":"部分可观测MDPs","slug":"部分可观测mdps","link":"#部分可观测mdps","children":[]},{"level":2,"title":"无衰减MDPs","slug":"无衰减mdps","link":"#无衰减mdps","children":[]}],"git":{"createdTime":1702351416000,"updatedTime":1702351416000,"contributors":[{"name":"hsqure","email":"hgocn2@gmail.com","commits":1}]},"filePathRelative":"docs/machine-learning/reinforcement-learning/reinforcement-learning/markov-decision-processes/markov-decision-processes.md"}');export{l as data};
