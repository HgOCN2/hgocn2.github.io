import{_ as s,r as o,o as i,c,b as e,e as a,w as r,d as n}from"./app-VitiyI7N.js";const u={},g=e("h1",{id:"自然语言处理",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#自然语言处理","aria-hidden":"true"},"#"),n(" 自然语言处理")],-1),d=e("p",null,"===",-1),h={href:"https://mp.weixin.qq.com/s/oMUASBSKe3xgGVLuQz7MGg",target:"_blank",rel:"noopener noreferrer"},m=e("strong",null,"Mixture-of-Denoisers",-1),_={href:"https://blog.csdn.net/haojiangcong/article/details/121565294",target:"_blank",rel:"noopener noreferrer"},f={href:"https://zhuanlan.zhihu.com/p/49271699",target:"_blank",rel:"noopener noreferrer"},p={href:"https://mp.weixin.qq.com/s/rxkHtRPMrEZPHzyq2UD4Fg",target:"_blank",rel:"noopener noreferrer"},x={href:"https://mp.weixin.qq.com/s/LaikrZUGlzn864ttdeyBLQ",target:"_blank",rel:"noopener noreferrer"};function k(q,b){const t=o("RouterLink"),l=o("ExternalLinkIcon");return i(),c("div",null,[g,e("ul",null,[e("li",null,[a(t,{to:"/docs/machine-learning/"},{default:r(()=>[n("返回上层目录")]),_:1})]),e("li",null,[a(t,{to:"/docs/machine-learning/natural-language-processing/natural-language-processing-introduction/natural-language-processing-introduction.html"},{default:r(()=>[n("自然语言处理概论")]),_:1})]),e("li",null,[a(t,{to:"/docs/machine-learning/natural-language-processing/natural-language/natural-language.html"},{default:r(()=>[n("自然语言")]),_:1})]),e("li",null,[a(t,{to:"/docs/machine-learning/natural-language-processing/language-model-and-chinese-word-segmentation/language-model-and-chinese-word-segmentation.html"},{default:r(()=>[n("语言模型和中文分词")]),_:1})]),e("li",null,[a(t,{to:"/docs/machine-learning/natural-language-processing/tf-idf/tf-idf.html"},{default:r(()=>[n("TF-IDF词频-逆文档频率")]),_:1})]),e("li",null,[a(t,{to:"/docs/machine-learning/natural-language-processing/word2vec/word2vec.html"},{default:r(()=>[n("word2vec")]),_:1})]),e("li",null,[a(t,{to:"/docs/machine-learning/natural-language-processing/seq2seq-and-attention-mechanism/seq2seq-and-attention-mechanism.html"},{default:r(()=>[n("Seq2Seq模型和Attention机制")]),_:1})]),e("li",null,[a(t,{to:"/docs/machine-learning/natural-language-processing/self-attention-and-transformer/self-attention-and-transformer.html"},{default:r(()=>[n("Self-Attention和Transformer")]),_:1})])]),d,e("p",null,[e("a",h,[m,n("击败GPT3，刷新50个SOTA！谷歌全面统一NLP范式"),a(l)])]),e("p",null,[e("a",_,[n("详解预训练模型 ——从词向量到GPT模型"),a(l)])]),e("p",null,[e("a",f,[n("从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史"),a(l)])]),e("p",null,[e("a",p,[n("从One-hot, Word embedding到Transformer，一步步教你理解Bert"),a(l)])]),e("p",null,[e("a",x,[n("从Word2Vec到Bert，聊聊词向量的前世今生（一）"),a(l)])])])}const L=s(u,[["render",k],["__file","natural-language-processing.html.vue"]]);export{L as default};
