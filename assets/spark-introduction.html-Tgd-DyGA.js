import{_ as p,r as t,o,c as l,b as a,e as s,w as i,d as n,a as c}from"./app-VitiyI7N.js";const d="/assets/spark-shell-install-1-Pf7XfRCL.png",h={},u=a("h1",{id:"spark简介",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#spark简介","aria-hidden":"true"},"#"),n(" Spark简介")],-1),k=a("h1",{id:"学习spark",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#学习spark","aria-hidden":"true"},"#"),n(" 学习spark")],-1),m=a("p",null,"建议从学习Spark官方文档开始：",-1),_={href:"https://spark.apache.org/docs/latest/quick-start.html",target:"_blank",rel:"noopener noreferrer"},b={href:"https://spark.apache.org/docs/latest/rdd-programming-guide.html",target:"_blank",rel:"noopener noreferrer"},v=a("p",null,"这里有对应的中译版：",-1),g={href:"http://coredumper.cn/index.php/2017/07/08/spark-quick-start/",target:"_blank",rel:"noopener noreferrer"},f={href:"http://coredumper.cn/index.php/2017/09/30/spark-programming-guide-1/",target:"_blank",rel:"noopener noreferrer"},x=a("hr",null,null,-1),y={href:"https://spark-reference-doc-cn.readthedocs.io/zh_CN/latest/index.html",target:"_blank",rel:"noopener noreferrer"},S={href:"https://spark.apachecn.org/",target:"_blank",rel:"noopener noreferrer"},q=a("hr",null,null,-1),j={href:"https://link.zhihu.com/?target=https%3A//github.com/JerryLead/SparkInternals",target:"_blank",rel:"noopener noreferrer"},L={href:"https://zhuanlan.zhihu.com/p/31307675",target:"_blank",rel:"noopener noreferrer"},D=c('<h1 id="linux安装单机版spark" tabindex="-1"><a class="header-anchor" href="#linux安装单机版spark" aria-hidden="true">#</a> Linux安装单机版spark</h1><p>只学习spark，还不学分布式的时候，可以先单机装spark，这时候不需要用到hadoop的，但是仍然需要java环境，所以要先安装java的jdk。</p><h2 id="下载并安装java-jdk" tabindex="-1"><a class="header-anchor" href="#下载并安装java-jdk" aria-hidden="true">#</a> 下载并安装java jdk：</h2><p>具体不细说了</p><h2 id="安装好java环境后-安装spark" tabindex="-1"><a class="header-anchor" href="#安装好java环境后-安装spark" aria-hidden="true">#</a> 安装好java环境后，安装spark</h2><p>因为是单机版，所以不需要安装先安装spark，直接上官网下载，地址：http://spark.apache.org/downloads.html ，没有hadoop环境就选择spark-2.3.2-bin-hadoop2.7就好了，点进下载链接后，里头有很多个镜像，选择一个能打开的下载就行。下载好之后，解压。</p><h2 id="打开shell-使用spark" tabindex="-1"><a class="header-anchor" href="#打开shell-使用spark" aria-hidden="true">#</a> 打开shell，使用spark</h2><p>cd到spark解压后的目录下，在我这里就是cd spark-2.3.2-bin-hadoop2.7，进入目录后，输入bin/spark-shell（<strong>如果在bin目录下，输入./spark-shell而不是spark-shell</strong>），这样就打开了scala shell，能够输入scala命令进行交互了。界面如下：</p><p><img src="'+d+`" alt="spark-shell-install-1"></p><p>给个交互的小例子：</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code>val <span class="token keyword">lines</span> <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">&quot;../data/README.md&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">lines</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
<span class="token keyword">lines</span><span class="token punctuation">.</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">lines</span><span class="token punctuation">.</span><span class="token function">first</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="退出spark-输入-quit" tabindex="-1"><a class="header-anchor" href="#退出spark-输入-quit" aria-hidden="true">#</a> 退出spark：输入“:quit”</h2><p>或者<code>:q</code>。</p><h2 id="使用scala和python的小例子" tabindex="-1"><a class="header-anchor" href="#使用scala和python的小例子" aria-hidden="true">#</a> 使用scala和python的小例子</h2><p>使用scala：在spark文件夹下输入：</p><div class="language-scala line-numbers-mode" data-ext="scala"><pre class="language-scala"><code>bin<span class="token operator">/</span>spark<span class="token operator">-</span>shell
<span class="token comment">//创建新的RDD，Resilient Distributed Dataset，弹性分布式数据集</span>
<span class="token keyword">var</span> lines <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span>“README<span class="token punctuation">.</span>md”<span class="token punctuation">)</span>  
<span class="token comment">//输出长度</span>
lines<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span>  
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>使用python：在spark文件夹下输入：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token builtin">bin</span><span class="token operator">/</span>pyspark   <span class="token operator">-</span><span class="token operator">-</span>如果配置了环境变量就可以直接用pyspark
textFile <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">&quot;README.md&quot;</span><span class="token punctuation">)</span>   <span class="token operator">-</span><span class="token operator">-</span>创建新的RDD
textFile<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token operator">-</span><span class="token operator">-</span>输出长度
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="spark2-0运行py文件的方法" tabindex="-1"><a class="header-anchor" href="#spark2-0运行py文件的方法" aria-hidden="true">#</a> spark2.0运行py文件的方法</h2><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>./bin/spark-submit filepath.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h1 id="spark相关操作" tabindex="-1"><a class="header-anchor" href="#spark相关操作" aria-hidden="true">#</a> spark相关操作</h1><h2 id="kill" tabindex="-1"><a class="header-anchor" href="#kill" aria-hidden="true">#</a> kill</h2><div class="language-shll line-numbers-mode" data-ext="shll"><pre class="language-shll"><code>yarn application -kill application_15xxxxx
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h1 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料" aria-hidden="true">#</a> 参考资料</h1>`,24),R={href:"https://blog.csdn.net/weixin_39750084/article/details/83661141",target:"_blank",rel:"noopener noreferrer"},w=a("p",null,'"Linux安装单机版spark"参考此博客。',-1);function E(F,N){const r=t("RouterLink"),e=t("ExternalLinkIcon");return o(),l("div",null,[u,a("ul",null,[a("li",null,[s(r,{to:"/docs/machine-learning/coding/big-data/spark/spark.html"},{default:i(()=>[n("返回上层目录")]),_:1})])]),k,m,a("p",null,[a("a",_,[n("Quick Start - Spark 2.1.1 Documentation"),s(e)])]),a("p",null,[a("a",b,[n("Spark Programming Guide"),s(e)])]),v,a("p",null,[a("a",g,[n("Spark官方文档－快速入门"),s(e)])]),a("p",null,[a("a",f,[n("Spark官方文档-Spark编程指南"),s(e)])]),x,a("p",null,[a("a",y,[n("Spark 2.2.x 中文官方参考文档"),s(e)])]),a("p",null,[a("a",S,[n("Spark 2.2.0 中文文档"),s(e)])]),q,a("p",null,[n("spark本身的知识点这里面讲的很全了："),a("a",j,[n("https://github.com/JerryLead/Sp.."),s(e)]),n("，from"),a("a",L,[n("基于spark大规模LR模型调优总结"),s(e)])]),D,a("ul",null,[a("li",null,[a("a",R,[n("Linux下安装单机版Spark并使用scala和python"),s(e)])])]),w])}const A=p(h,[["render",E],["__file","spark-introduction.html.vue"]]);export{A as default};
