import{_ as a,r as t,o as s,c as i,b as e,e as o,w as c,d as n}from"./app-VitiyI7N.js";const h="/assets/rl-control-example-V72gW4Wo.png",_={},p=e("h1",{id:"基于强化学习的控制",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#基于强化学习的控制","aria-hidden":"true"},"#"),n(" 基于强化学习的控制")],-1),d=e("p",null,"===",-1),u={href:"https://zhuanlan.zhihu.com/p/641918952",target:"_blank",rel:"noopener noreferrer"},f=e("p",null,"23年6月26号，ETH在arxiv挂出了他们最新的论文，视频暂时还没有放出来。推测这篇应该是投了science robotics。",-1),m=e("p",null,"我们趁热打铁来分析一波这篇论文。",-1),g=e("p",null,"https://arxiv.org/pdf/2306.14874.pdf",-1),b=e("p",null,"“什么mpc和rl不实用，都是老一代思想”，mpc我不懂，rl在真实的运动控制上还是差的很远的。",-1),k=e("p",null,"我读博开始那会也是这么认为的，想想都觉得不靠谱，直到ETH的rl狗给所有做rl控制的人打了一剂强心针",-1),x=e("p",null,"eth基本上现在每年一篇science robotics，基本上都有用rl，算是现在四足rl标杆了。",-1),C={href:"https://arxiv.org/abs/2008.12228",target:"_blank",rel:"noopener noreferrer"},w={href:"https://www.zhihu.com/question/402251886/answer/2449239868",target:"_blank",rel:"noopener noreferrer"},L=e("p",null,"这个Actor Critic网络是不是颇有点控制中反馈的意味：",-1),M=e("p",null,[e("img",{src:h,alt:"rl-control-example"})],-1),z={href:"https://www.zhihu.com/question/402251886/answer/1294806970",target:"_blank",rel:"noopener noreferrer"},E={href:"https://zhuanlan.zhihu.com/p/68960762",target:"_blank",rel:"noopener noreferrer"},P={href:"https://github.com/tobiasfshr/deep-reinforcement-learning-drone-control",target:"_blank",rel:"noopener noreferrer"},R={href:"https://zhuanlan.zhihu.com/p/144544347",target:"_blank",rel:"noopener noreferrer"};function T(v,H){const l=t("RouterLink"),r=t("ExternalLinkIcon");return s(),i("div",null,[p,e("ul",null,[e("li",null,[o(l,{to:"/docs/machine-learning/artificial-general-intelligence/autopilot/control/control.html"},{default:c(()=>[n("返回上层目录")]),_:1})])]),d,e("p",null,[e("a",u,[n("Science Robotics 预定？ETH最新四足“跑酷”"),o(r)])]),f,m,g,b,k,x,e("p",null,[n("谢谢分享，我还是第一次看到这个工作。RL做机器人我一直知道，做的最好的我以前以为是DeepMind的"),e("a",C,[n("Towards General and Autonomous Learning of Core Skills: A Case Study in Locomotion"),o(r)]),n("。DeepMind这个工作没有任何MPC，他就靠设计一个多层次的reward function然后不断training就出来了。DeepMind一直是致力于通用智能，我觉得他们搞的hierarchical RL来做控制方向还是很对的。相比之下ETH这个结构复杂的多，还需要MPC和一堆传统控制用的方法，效果上来说似乎ETH的好一点（从地形上看）。但是似乎还是比不过波士顿动力的人工调的MPC。")]),e("p",null,[n("来自："),e("a",w,[n("在实际工程系统中，MPC太复杂，难以做到realtime。因此都利用数据，MPC对比RL还有优势么?"),o(r)])]),L,M,e("p",null,[n("来自："),e("a",z,[n("在实际工程系统中，MPC太复杂，难以做到realtime。因此都利用数据，MPC对比RL还有优势么?"),o(r)])]),e("p",null,[e("a",E,[n("09-基于强化学习的固定翼无人机着陆飞行控制仿真"),o(r)])]),e("p",null,[n("其中使用的代码来自"),e("a",P,[n("deep-reinforcement-learning-drone-control"),o(r)]),n("。")]),e("p",null,[e("a",R,[n("【重磅综述】如何在少量尝试下学习机器人强化学习控制"),o(r)])])])}const V=a(_,[["render",T],["__file","reinforcement-learning-based-control.html.vue"]]);export{V as default};
