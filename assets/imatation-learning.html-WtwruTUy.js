import{_ as o,r as l,o as c,c as s,b as n,e as i,w as t,d as e}from"./app-VitiyI7N.js";const m={},d=n("h1",{id:"模仿学习",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#模仿学习","aria-hidden":"true"},"#"),e(" 模仿学习")],-1),g=n("p",null,"===",-1),h={href:"https://aistudio.csdn.net/63a2b180c9ce291554e20b4d.html",target:"_blank",rel:"noopener noreferrer"},_={href:"https://blog.csdn.net/tianjuewudi/article/details/122169309",target:"_blank",rel:"noopener noreferrer"};function u(f,p){const a=l("RouterLink"),r=l("ExternalLinkIcon");return c(),s("div",null,[d,n("ul",null,[n("li",null,[i(a,{to:"/docs/machine-learning/reinforcement-learning/reinforcement-learning.html"},{default:t(()=>[e("返回上层目录")]),_:1})]),n("li",null,[i(a,{to:"/docs/machine-learning/reinforcement-learning/imatation-learning/imatation-learning-introduction/imatation-learning-introduction.html"},{default:t(()=>[e("模仿学习介绍")]),_:1})]),n("li",null,[i(a,{to:"/docs/machine-learning/reinforcement-learning/imatation-learning/behavior-cloning/behavior-cloning.html"},{default:t(()=>[e("BC行为克隆:Behavior Cloning")]),_:1})]),n("li",null,[i(a,{to:"/docs/machine-learning/reinforcement-learning/imatation-learning/inverse-reinforcement-learning/inverse-reinforcement-learning.html"},{default:t(()=>[e("IRL逆强化学习Inverse Reinforcement Learning")]),_:1})]),n("li",null,[i(a,{to:"/docs/machine-learning/reinforcement-learning/imatation-learning/generative-adversarial-imitation-learning/Generative-Adversarial-Imitation-Learning.html"},{default:t(()=>[e("GAIL生成式对抗模仿学习: Generative Adversarial Imitation Learning NeurIPS2016")]),_:1})]),n("li",null,[i(a,{to:"/docs/machine-learning/reinforcement-learning/imatation-learning/paper/paper.html"},{default:t(()=>[e("论文")]),_:1})])]),g,n("p",null,[n("a",h,[e("【强化学习】模仿学习：行为克隆"),i(r)])]),n("p",null,[n("a",_,[e("模仿学习与强化学习的结合（原理讲解与ML-Agents实现）"),i(r)])])])}const L=o(m,[["render",u],["__file","imatation-learning.html.vue"]]);export{L as default};
