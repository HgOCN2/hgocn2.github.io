import{_ as o,r as l,o as s,c as i,b as e,e as a,w as r,d as n}from"./app-VitiyI7N.js";const f={},d=e("h1",{id:"self-attention机制和transformer",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#self-attention机制和transformer","aria-hidden":"true"},"#"),n(" Self-Attention机制和Transformer")],-1);function c(m,u){const t=l("RouterLink");return s(),i("div",null,[d,e("ul",null,[e("li",null,[a(t,{to:"/docs/machine-learning/natural-language-processing/natural-language-processing.html"},{default:r(()=>[n("返回上层目录")]),_:1})]),e("li",null,[a(t,{to:"/docs/machine-learning/natural-language-processing/self-attention-and-transformer/attention-is-all-you-need/attention-is-all-you-need.html"},{default:r(()=>[n("Transformer: Attention Is All You Need NIPS2017")]),_:1})]),e("li",null,[a(t,{to:"/docs/machine-learning/natural-language-processing/self-attention-and-transformer/transformer-tf2-demo-code-explain/transformer-tf2-demo-code-explain.html"},{default:r(()=>[n("Transformer模型tensorflow2.0官网demo代码解读")]),_:1})]),e("li",null,[a(t,{to:"/docs/machine-learning/natural-language-processing/self-attention-and-transformer/transformer-details/transformer-details.html"},{default:r(()=>[n("Transformer的细节问题")]),_:1})])])])}const g=o(f,[["render",c],["__file","self-attention-and-transformer.html.vue"]]);export{g as default};
