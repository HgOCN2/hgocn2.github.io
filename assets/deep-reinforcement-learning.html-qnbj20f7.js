import{_ as m,r as e,o as p,c as r,b as s,e as t,w as c,d as a,a as l}from"./app-VitiyI7N.js";const h="/assets/learning-map-kf1co3x6.png",o="/assets/flappybird-6-ChCR4Q.png",g="/assets/DQN-eOWRT6eI.png",u="/assets/deep-Q-learning-with-experience-relay-WXHSNtBZ.png",d="/assets/Double-DQN-WzTNc2pP.png",y="/assets/Double-DQN-algorithm-svOAJWAA.png",v="/assets/Dueling-DQN-qk3ifc1L.png",b="/assets/Dueling-DQN-2-416g_ScJ.png",D="/assets/Prioritized-Experience-Replay-KuZ_n8tM.png",x="/assets/Double-DQN-with-proportional-prioritization-M5EzJjiU.png",Q="/assets/Q-learning-to-Double-DQN-h07_q2Rc.png",w="/assets/Rainbow-VS-DQNs-feaUzBks.png",_="/assets/PG-to-DDPG-lekuw3Jg.png",z="/assets/DDPG-modeling-methods-Tp4lDa1r.png",f="/assets/DDPG-algorithm-KNEbkwFQ.png",N="/assets/PG-to-A3C-ZS6jBQk9.png",A="/assets/A3C-algorithm-VAImWWPI.png",k="/assets/PG-to-A2C-OkkSqy-1.png",P="/assets/PG-to-DDPG-and-A2C-MiytG9Xo.png",E="/assets/TRPO-WpXX2vQ9.png",R="/assets/PPO-FB8MTPOR.png",M="/assets/PPO-algorithm-SxOrg-YO.png",L={},B=s("h1",{id:"深度强化学习",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#深度强化学习","aria-hidden":"true"},"#"),a(" 深度强化学习")],-1),C=l('<li><a href="#%E6%9C%AC%E7%AB%A0%E5%9C%A8%E5%AD%A6%E4%B9%A0%E5%9C%B0%E5%9B%BE%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE">本章在学习地图中的位置</a></li><li><a href="#%E6%9C%AC%E7%AB%A0%E7%AE%80%E4%BB%8B">本章简介</a><ul><li><a href="#%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B">深度强化学习简介</a></li><li><a href="#%E8%AE%B2%E8%A7%A3%E6%B5%81%E7%A8%8B">讲解流程</a></li></ul></li><li>[Value-based DRL](#Value-based DRL) <ul><li><a href="#DQN">DQN</a></li><li>[Double DQN](#Double DQN)</li><li>[Dueling DQN](#Dueling DQN)</li><li>[Prioritized Experience Replay](#Prioritized Experience Replay)</li><li>[Q learning到各DQN变体的演变](#Q learning到各DQN变体的演变)</li><li><a href="#Rainbow">Rainbow</a></li></ul></li><li>[Policy-based DRL](#Policy-based DRL) <ul><li><a href="#DPG">DPG</a></li><li><a href="#DDPG">DDPG</a></li><li><a href="#A3C">A3C</a></li><li><a href="#A2C">A2C</a></li><li><a href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%88%B0%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5DRL%E7%9A%84%E6%BC%94%E5%8C%96">策略梯度到基于策略DRL的演化</a></li></ul></li><li>[Trust Region based DRL](Trust Region based DRL) <ul><li><a href="#%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D">简要介绍</a></li><li><a href="#%E4%B8%80%E4%BA%9B%E5%9F%BA%E7%A1%80">一些基础</a></li><li><a href="#TRPO">TRPO</a></li><li><a href="#PPO">PPO</a></li><li><a href="#%E5%85%B6%E4%BB%96%E4%BF%A1%E8%B5%96%E5%9F%9F%E7%AE%97%E6%B3%95">其他信赖域算法</a></li></ul></li>',5),T=l('<h1 id="本章在学习地图中的位置" tabindex="-1"><a class="header-anchor" href="#本章在学习地图中的位置" aria-hidden="true">#</a> 本章在学习地图中的位置</h1><p><img src="'+h+'" alt="learning-map"></p><h1 id="本章简介" tabindex="-1"><a class="header-anchor" href="#本章简介" aria-hidden="true">#</a> 本章简介</h1><h2 id="深度强化学习简介" tabindex="-1"><a class="header-anchor" href="#深度强化学习简介" aria-hidden="true">#</a> 深度强化学习简介</h2><ul><li><p>深度学习+强化学习=深度强化学习</p></li><li><p>使用深度神经网络作为强化学习的函数近似器</p><ul><li>函数近似器：从状态到值函数的映射关系</li><li>函数近似器：从状态到策略的映射关系</li></ul></li><li><p>深度学习本质上是一种表示学习，它负责将原始数据表达为和问题相关的特征</p></li><li><p>例：flappybird</p><ul><li><p>图像输入：DRL</p><p>不要迷信深度强化学习，深度学习适合的场景为输入特征和真实特征相距比较远的情况下</p></li><li><p>坐标输入：RL</p><p>如果是坐标输入的话，那就不需要深度强化学习，强化学习就好</p></li></ul></li></ul><p><img src="'+o+'" alt="flappybird"></p><h2 id="讲解流程" tabindex="-1"><a class="header-anchor" href="#讲解流程" aria-hidden="true">#</a> 讲解流程</h2><ul><li>基于值函数：从DQN到Rainbow</li><li>基于策略 <ul><li>策略梯度框架</li><li>信赖域优化</li></ul></li></ul><p>本节课会大量引用文献。每说一个问题，就会把其最原始的paper展现给大家。为了方便大家的文献阅读，尽量使用和原文类似的英文表达。</p><h1 id="value-based-drl" tabindex="-1"><a class="header-anchor" href="#value-based-drl" aria-hidden="true">#</a> Value-based DRL</h1><h2 id="dqn" tabindex="-1"><a class="header-anchor" href="#dqn" aria-hidden="true">#</a> DQN</h2><p>基于值函数的深度强化学习。这里讲基于值函数的方法是如何拓展到深度强化学习的。</p><p>深度强化学习第一个工作，也是最具有代表性的工作，也是深度强化学习第一个引入中国的工作，就是DQN。</p><p>DQN有两个版本，一个是在NIPS2013，一个是在Nature2015上，都是DeepMind做的。</p><ul><li>Playing Atari with Deep Reinforcement Learning (NIPS2013)</li><li>Human-level control through deep reinforcement learning (Nature2015)</li><li>DQN是非常重要的工作，创新点在于真正从实际意义上，给大家证明了一件事，重要意义在于： <ul><li>以前做RL的人觉得：不用函数近似无法解决大规模问题，用函数近似训练又不稳定，总是不能找到平衡。DQN是第一个告诉人们，能够通过深度神经网络做函数近似。</li><li>DQN首次证明了能够通过原始像素点（raw pixels）作为输入就能解决游戏问题</li><li>最关键的是，DQN对所有游戏通用。大部分游戏只需要微调一下奖励函数的形式，其他的比如输入都不用改，就可以把游戏打好。打Atari游戏并不是很难的工作，加入一些规则，用传统的方法甚至可以比DQN打得更好。DQN最关键的一点就是不需要你做人为的设计规则，来一个游戏就能打一个游戏，甚至连模型都不用调，只需要重新训练一遍就可以了，DQN最重要的工作就在于这一点。</li></ul></li></ul><p><strong>DQN的关键特点</strong></p><ul><li>Q learning + DNN</li><li>Experience Replay</li><li>Target Network</li></ul><p><img src="'+g+'" alt="DQN"></p><ul><li><p>一开始学习<strong>Q学习</strong>的时候，它是一个基于表格的方法</p></li><li><p>后来我们学了一个基于<strong>函数近似</strong>的方法</p><ul><li>函数近似有两种，一种是线性的</li><li>另一种是<strong>非线性</strong>的 <ul><li>非线性的也有其他的方法，比如决策树等等</li><li>最关键的是之前所提到的，用<strong>神经网络</strong>做函数近似</li></ul></li></ul></li><li><p><strong>深度神经网络DQN(NIPS2013)</strong></p><ul><li>怎么解决训练的问题呢？引入了<strong>Experience Replay</strong>，这是经验回放。经验回放是什么东西？它是在电脑里开辟内存存储区，比如就是个矩阵或者队列等等。开辟好存储区，然后每次和环境交互的时候，会产生一些transition，指的是状态，动作到后继状态等状态的一次跳转，把这些transition存到存储区里面，然后每次训练的时候，会对存储区做均匀的采样，用所采样的样本去训练。它有什么好处？最大的好处是，强化学习有个关键的点在于，训练数据的样本之间有高度的相关性，比如你和某一个人下棋下了一个小时，那么这个期间采样的所有数据都是和这个人下棋，数据之间其实是不满足独立同分布的，导致训练过程会朝着和这个人对弈的结果去收敛。后果就是，当和另一个人下棋，立马就完蛋了。所以，为了打破数据的相关性，就把数据先存起来，然后再去回放，随机抽取。，这样就不会只是某个人的了，避免了完全跟随某个人的风格。可能大家觉得这个思路比较简单，但里面隐藏了比较深的因素：如果把存的样本再去回放，其实必定对应着off-policy离策略的学习方法，原因是所存的样本对应的策略已经是过去的策略了，而参数的目标是求现在参数的策略，而存的样本是过去那套参数的策略，所以训练的样本和实际更新的网络，必定是离策略的。在离策略的情况下，选择的Experience Replay方法为什么会和DQN去结合，和Q学习结合，而不是和SARSA结合，最大的原因是在于它一定只能用off-policy的方法去做，而off-policy方法如果是TD(0)的话，是不需要重要性采样这个因子的，所以可以很完美的结合。</li></ul></li><li><p><strong>深度神经网络DQN(Nature2015)</strong></p><p>这里又加了一个改动，加了一个<strong>target nerwork</strong>，这个是什么东西，下面结合算法讲解。就是在构建TD目标值的时候，不要用现在的这个Q去构建目标值，因为现在这个Q函数一直处于更新的状态，一直在波动，如果目标值也随之波动，会导致训练很不稳定，所以它搞了一个目标网络，专门负责构建TD目标值。而目标网络怎么去更新呢？就是现在更新的网络每更新100步了，然后把现在这个网络拷贝到目标网络，然后这个目标网络在下个100步中保持不变，一直作为Q网络更新的指导（TD目标值），然后再100步再去更新。</p></li></ul><p><strong>算法：deep Q-learning with experience relay</strong></p><p><img src="'+u+'" alt="deep-Q-learning-with-experience-relay"></p><p>这个算法</p>',22),G=s("ul",null,[s("li",null,"是Q学习，带ε的贪婪行为策略。"),s("li",null,"以一阶的ε策略选一个最大的"),s("li",null,[a("然后执行动作后就可以和环境进行交互，得到下一个奖励值和下一个状态。 "),s("ul",null,[s("li",null,"这里还对状态做了一个预处理，把预处理后的图像做一个状态")])]),s("li",null,"transition对应t时刻的状态和动作和奖励，然后再跳转到下一个状态，这叫一个transition。"),s("li",null,"把这个transition存到memory里面，"),s("li",null,"DNQ对应两部份，第一个部分就是从此往前，对应一个交互过程，就是agent用行为策略和environment进行交互。接下来就是学习。"),s("li",null,"怎么更新呢？并不是采用transition更新，而是从memory里面采样一个batch"),s("li",null,[a("然后首先更新TD目标值 "),s("ul",null,[s("li",null,"如果后继状态是终止状态，那么奖励就是该状态的即时奖励"),s("li",null,[a("否则就是奖励加上Q学习里面的目标值，这个目标值用的是一个目标"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mover",{accent:"true"},[s("mi",null,"Q"),s("mo",null,"^")])]),s("annotation",{encoding:"application/x-tex"},"\\hat{Q}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.1412em","vertical-align":"-0.1944em"}}),s("span",{class:"mord accent"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.9468em"}},[s("span",{style:{top:"-3em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord mathnormal"},"Q")]),s("span",{style:{top:"-3.2523em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"accent-body",style:{left:"-0.1667em"}},[s("span",{class:"mord"},"^")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1944em"}},[s("span")])])])])])])]),a("网络，这个"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mover",{accent:"true"},[s("mi",null,"Q"),s("mo",null,"^")])]),s("annotation",{encoding:"application/x-tex"},"\\hat{Q}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.1412em","vertical-align":"-0.1944em"}}),s("span",{class:"mord accent"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.9468em"}},[s("span",{style:{top:"-3em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord mathnormal"},"Q")]),s("span",{style:{top:"-3.2523em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"accent-body",style:{left:"-0.1667em"}},[s("span",{class:"mord"},"^")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1944em"}},[s("span")])])])])])])]),a("网络和前面更新的Q网络一样又不一样，参数不一样，用目标Q网络更新的会更缓慢一点，因为要用比较稳定的网络构建目标值。")])])]),s("li",null,"函数近似的本质，就是用输入拟合一个目标值，然后最小化均方误差，用梯度下降的方式，去更新函数近似中的参数。"),s("li",null,"然后每隔C步把目标网络更新一遍。")],-1),O=s("p",null,"这就是DQN，最重要的是这俩个技术：Experience Replay和target nerwork。这两个技术真正让DQN运转起来了。很多人都会想到有了深度学习和强化学习，我可以把它结合在一起，但是能做大训练稳定的，只有DQN，这是DeepMind做的最大的贡献。",-1),S=s("p",null,"下面说两个DQN的拓展，大家也基本都会去用，因为被证明在一般意义上，都会比DQN要好。",-1),I=s("h2",{id:"double-dqn",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#double-dqn","aria-hidden":"true"},"#"),a(" Double DQN")],-1),V=s("ul",null,[s("li",null,[s("p",null,[s("strong",null,"把Double Q学习和DQN结合，就是Double DQN")])]),s("li",null,[s("p",null,"Deep Reinforcement Learning with Double Q-learning (AAAI2016)")]),s("li",null,[s("p",null,[a("Q学习中存在"),s("strong",null,"过估计"),a("（Q学习本身存在的缺陷）")]),s("ul",null,[s("li",null,[s("p",null,[a("Q学习中的TD目标值"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"r"),s("mo",null,"+"),s("mi",null,"γ"),s("msub",null,[s("mo",null,[s("mtext",null,"max")]),s("mi",null,"a")]),s("mi",null,"Q"),s("mo",{stretchy:"false"},"("),s("msup",null,[s("mi",null,"s"),s("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")]),s("mo",{separator:"true"},","),s("mi",null,"a"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"r+\\gamma \\mathop{\\text{max}}_aQ(s',a)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6667em","vertical-align":"-0.0833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0019em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.05556em"}},"γ"),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mop"},[s("span",{class:"mop"},[s("span",{class:"mord text"},[s("span",{class:"mord"},"max")])]),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"a")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"Q"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7519em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"′")])])])])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"a"),s("span",{class:"mclose"},")")])])]),a("中"),s("strong",null,"存在max操作"),a("。")]),s("p",null,"如果Q是很精确的情况下，max没什么问题，但关键Q是一个估计值，就会存在噪声，取max会把噪声大的部分取到，就会引入正向偏差。")]),s("li",null,[s("p",null,[a("这会引入一个"),s("strong",null,"正向的偏差")]),s("p",null,"例子：对于状态s下，如果对于所有的a，真实的q(s,a)均为0，但是估计值由于不精确，会导致有些大于0，有些小于0。虽然平均值接近真实值。但对估计值取最大，就会选择最大的噪声值，导致某一个动作被凸显出来，会引入一个正向的偏差。")]),s("li",null,[s("p",null,[a("因此"),s("strong",null,"建模两个Q网络"),a("，一个用于选动作，一个用于评估动作")])])]),s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",null,"r"),s("mo",null,"+"),s("mi",null,"γ"),s("msup",null,[s("mi",null,"Q"),s("mi",null,"B")]),s("mo",{stretchy:"false"},"("),s("msup",null,[s("mi",null,"s"),s("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")]),s("mo",{separator:"true"},","),s("mtext",null,"arg "),s("msub",null,[s("mo",null,[s("mtext",null,"max")]),s("mi",null,"a")]),s("msup",null,[s("mi",null,"Q"),s("mi",null,"A")]),s("mo",{stretchy:"false"},"("),s("msup",null,[s("mi",null,"s"),s("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")]),s("mo",{separator:"true"},","),s("mi",null,"a"),s("mo",{stretchy:"false"},")"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"}," r+\\gamma Q^B(s',\\text{arg }\\mathop{\\text{max}}_aQ^A(s',a)) ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6667em","vertical-align":"-0.0833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.1413em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.05556em"}},"γ"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"Q"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8913em"}},[s("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05017em"}},"B")])])])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8019em"}},[s("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"′")])])])])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord text"},[s("span",{class:"mord"},"arg ")]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mop"},[s("span",{class:"mop"},[s("span",{class:"mord text"},[s("span",{class:"mord"},"max")])]),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"a")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"Q"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8913em"}},[s("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"A")])])])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"s"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8019em"}},[s("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"′")])])])])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"a"),s("span",{class:"mclose"},"))")])])])])]),s("p",null,[a("这样就会有有一些抵消，可以减缓这个过程，比如接着上面的例子，有些Q噪声虽然大于0，沿着大于零选了动作，比如a1被估计的大于0且是最大，则根据"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"Q"),s("mi",null,"A")])]),s("annotation",{encoding:"application/x-tex"},"Q^A")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0358em","vertical-align":"-0.1944em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"Q"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8413em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"A")])])])])])])])])])]),a("网络，"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"a"),s("mn",null,"1")])]),s("annotation",{encoding:"application/x-tex"},"a_1")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"a"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a("就被选择，但是对于另外一个Q网络"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"Q"),s("mi",null,"B")])]),s("annotation",{encoding:"application/x-tex"},"Q^B")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0358em","vertical-align":"-0.1944em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"Q"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8413em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05017em"}},"B")])])])])])])])])])]),a("来评估它的时候，则对于"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"a"),s("mn",null,"1")])]),s("annotation",{encoding:"application/x-tex"},"a_1")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"a"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a("的评估可能是小于0的，因为"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"Q"),s("mi",null,"B")])]),s("annotation",{encoding:"application/x-tex"},"Q^B")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0358em","vertical-align":"-0.1944em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"Q"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8413em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05017em"}},"B")])])])])])])])])])]),a("并不存在max操作，相互会抵消，则不存在过估计。")])])],-1),j=l('<p><strong>Double DQN算法：</strong></p><p><img src="'+d+'" alt="Double-DQN"></p><p>继续将上面的图往外扩展，DQN结合Double Q Learning，就得到了Double DQN。</p><p>注意在实现的时候，之前提到DQN里有一个target网络，有一个更新的网络。既然DQN本身就有两个网络，那再Double DQN中有一个最简单的方法，把target网络也用来更新，则直接就构成了Double DQN。之前DQN中的target网络是用现在的网络去拷贝到target网络，现在就不拷贝了，各有各的学习方法和学习参数，然后交替更新，你更新我，我更新你，所以在Double DQN实现的时候，对原来的DQN改动的很小。因为你DQN本身也建模了两个Q网络，现在Double DQN也建模了两个Q网络。</p><p>下面给的算法片段，并不是Double DQN，而是Double Q学习，用的还是表格法。但是把里面的更新公式，换成梯度下降的更新公式，就可以直接得到Double DQN。</p><p><img src="'+y+'" alt="Double-DQN-algorithm"></p><h2 id="dueling-dqn" tabindex="-1"><a class="header-anchor" href="#dueling-dqn" aria-hidden="true">#</a> Dueling DQN</h2><ul><li>Dueling Network Architectures for Deep Reinforcement (ICML2016)</li><li>核心是：将Q函数分解成V函数和优势(Advantage)函数</li></ul><p><img src="'+v+'" alt="Dueling-DQN"></p><ul><li><p>上图中的第一幅图是DQN，通过卷积层，将输入变成Q值</p></li><li><p>上图中的第二幅图是Dueling DQN，将输出分成了两部分：</p><ul><li>预测V值，即预测每个状态的V函数</li><li>预测每个状态的每个动作下的优势函数</li></ul><p>把这两者结合起来，变成一个Q，更新的时候还是和DQN一样，构建一个TD目标值去更新Q。主要是在结构上，做了这么一个拆分</p></li></ul><p>这样子有什么好处？</p><p>优势是：</p><ul><li><p>对于很多状态并不需要估计每个动作的值，增加了V函数的学习机会</p><p>Q函数其实比V函数多一个维度，多一个动作的维度，一旦多一个维度，这个量级就已经不是一个量级了。假设总共有10个状态，有10个动作，当你采样时，每个状态只采样了2个动作，则更新Q的时候，每个状态只更新了2个，即只有2个Q会更新的比较好，其他8个Q都更新的不是很好，但用DQN的话，每个Q之间都是分离的，因为每一个Q会对应每一个输出，但这样会把它强行归纳到一个V函数，所以不管那一次采样到什么动作，对应的状态至少都是一样的，所以会大大增加V函数学习的机会。</p></li><li><p>V函数的泛化性能好，当有新动作加入时，并不需要重新学习</p><p>对于DQN，要加入一个新动作，q,s,a都需要从初始化开始重新学习。但如果用Dueling DQN结构的方式，就会从V做一个baseline（基准线）去学习。这样学习起来会非常的简单。</p><p>在本节开头的那篇paper中，V函数学了路径的整体趋势，优势函数Advantage就学了每一个动作导致预测轨迹的改变量，看上去比较直观。</p></li><li><p>减少了Q函数由于状态和动作维度差异导致的噪声和突变</p></li></ul><p><img src="'+b+'" alt="Dueling-DQN-2"></p><p>上图的解释：</p><ul><li>在Q Learning中，把Q分解为V函数和A优势函数，做成Advantage Learning</li><li>然后结合DQN，得到Dueling DQN</li></ul><h2 id="prioritized-experience-replay" tabindex="-1"><a class="header-anchor" href="#prioritized-experience-replay" aria-hidden="true">#</a> Prioritized Experience Replay</h2><p>之前的Double DQN和Dueling DQN都是对DQN的结构，去做一些改变。</p><p>既然提了Experience Replay，那能不能把把原来的均匀采样换一种方式，即把Experience Replay做优化？</p><ul><li><p>Prioritized Experience Replay (ICLR 2016)</p><p>在2016年这个会议上，提出了一个优先级的经验回放</p></li><li><p>DQN算法的一个重要改进是Experience Replay ，训练时从Memory中均匀采样</p></li><li><p>而Prioritized Experience Replay就是维护了一个带优先级的Experience Replay采样</p><ul><li><p>不同的Experience的权重不同</p><p>在一些比较复杂的环境里，和环境进行交互，有一些很重要的样本，很难去采样到。好不容易采样到一次，则希望这次的采样能带来更多的反馈。就比如好不容易你做成了一件事，你不能把这种事和你平时的吃饭走路搞成一个等级。这里也是一样的，对每一个Experience打不同的权重</p></li><li><p>用TD误差去衡量权重</p><p>TD误差越大，表示目前来说，解决的不是很好，而这个不太好的样本，我就认为是很重要的，那就给它更高的权重。对于权重越大的，就先把它采样出来</p></li><li><p>需要使用sum-tree以及binary heap data structure去实现</p><p>由于这个不是均匀采样了，所以为了降低复杂度，实现起来需要用相应的数据结构去实现</p></li><li><p>新的transition的TD误差会被设置为最大</p><p>每一次为了保证每一个transition都能被更新，所以对于新发生的transition，由于都还没有拿来更新，比如DQN里面，每次得到transition都是先存到memory，还没有计算TD误差。这个时候怎么设置TD误差呢？会把它设为最大，以保证新的transition至少会被更新一次</p></li><li><p>类似于DP中的优先清理</p><p>对于不同的状态给予了不同的优先级</p></li></ul></li><li><p>DQN中的Experience Replay使得更新不受限于实际经验的<strong>顺序</strong>，即尽量打破相邻经验之间的相关性，更新和实际环境交互的顺序没有关系。Prioritized Experience Replay使得更新不受限于实际经验的<strong>频率</strong>，如果是均匀采样，按道理，和环境交互的频率越多，在memory中存的越多，均匀采样的环境更新的也就越多。而现在并不是交互的频率越多，更新就越多，而是TD误差越大，更新就越多。所以说和频率就没有关系了。</p></li></ul><p>具体使用时会存在一些问题</p><ul><li><p>TD误差对噪声非常敏感</p></li><li><p>TD误差小的transition长时间不更新</p><p>拿到一个transition，其TD误差为零，就把它存到优先级的最后面，那么这个transition存到最后面有可能就一辈子不更新了。由于参数是发生变化的，有可能这个transition是非常有用的，但是你已经把这个transition存到了最后面，你就永远都拿不出来了。所以这也是一个弊端。</p></li><li><p>过分关注TD误差大的transition，导致丧失了样本多样性</p><p>在特殊情况下（机器人摔倒了），可能会有一个TD误差非常大，你反复拿这个TD误差非常大的transition去更新，导致其他的没有去更新，会导致过拟合，会对这个TD误差特别大的做得很好，其他的都做得不是很好。</p></li><li><p>使用某种分布采样了Experience，会引入Bias</p><p>在估计某个变量的时候，用了一种特殊的采样方法去采，其实就改变了采样的分布，就需要重要性采样来纠正。</p></li></ul><p>解决方法</p>',23),q=s("ul",null,[s("li",null,[s("p",null,[a("两种变体："),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"p"),s("mi",null,"i")]),s("mo",null,"="),s("mi",{mathvariant:"normal"},"∣"),s("msub",null,[s("mi",null,"δ"),s("mi",null,"i")]),s("mi",{mathvariant:"normal"},"∣"),s("mo",null,"+"),s("mi",null,"ϵ")]),s("annotation",{encoding:"application/x-tex"},"p_i=|\\delta_i|+\\epsilon")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.625em","vertical-align":"-0.1944em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"p"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},"∣"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03785em"}},"δ"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0379em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mord"},"∣"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"ϵ")])])]),a("或者"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"p"),s("mi",null,"i")]),s("mo",null,"="),s("mn",null,"1"),s("mi",{mathvariant:"normal"},"/"),s("mi",null,"r"),s("mi",null,"a"),s("mi",null,"n"),s("mi",null,"k"),s("mo",{stretchy:"false"},"("),s("mi",null,"i"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"p_i=1 / rank(i)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.625em","vertical-align":"-0.1944em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"p"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},"1/"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03148em"}},"ank"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"i"),s("span",{class:"mclose"},")")])])])]),s("p",null,"把优先级不再用TD误差去衡量，用另外两种去解决前三个问题。"),s("p",null,[a("第一个方法就是加了噪声，"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",{mathvariant:"normal"},"∣"),s("msub",null,[s("mi",null,"δ"),s("mi",null,"i")]),s("mi",{mathvariant:"normal"},"∣")]),s("annotation",{encoding:"application/x-tex"},"|\\delta_i|")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},"∣"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03785em"}},"δ"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0379em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mord"},"∣")])])]),a("是TD误差的绝对值，在其上加了一个噪声，作为优先级去存到memory的队列里面。")]),s("p",null,"第二个方法是用排序，对每一个TD误差去排序，排序排到最高（第一个），对排序的倒数作为优先级，这样的话，即使误差再大，那也就是第一名第二名，通过排序，那优先级也不过是1/1, 1/2，这样抗干扰就比较强。")]),s("li",null,[s("p",null,"加入重要性采样来消除Bias"),s("p",null,"这个来解决上面的第四个（最后一个）问题。在估计某个变量的时候，用了一种特殊的采样方法去采，其实就改变了采样的分布，就需要重要性采样来纠正。")])],-1),J=s("p",null,[s("img",{src:D,alt:"Prioritized-Experience-Replay"})],-1),F=s("p",null,"上图说明：DQN结合带优先级的经验回放（由之前动态规划中的优先清理得到），就得到了带优先级经验回放的DQN。",-1),K=s("p",null,"下面是带优先级经验回放的DQN算法片段，和DQN差不多",-1),W=s("p",null,[s("img",{src:x,alt:"Double-DQN-with-proportional-prioritization"})],-1),U=s("ul",null,[s("li",null,[a("for中第一行，p其实是每一个transition的优先级，然后可以算出每一个transition的概率"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"P"),s("mo",{stretchy:"false"},"("),s("mi",null,"j"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"P(j)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.05724em"}},"j"),s("span",{class:"mclose"},")")])])]),a("，然后通过这个概率取采样")]),s("li",null,"采样完后，计算重要性采样的权重wj，进行修正"),s("li",null,[a("计算TD误差"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"δ"),s("mi",null,"j")])]),s("annotation",{encoding:"application/x-tex"},"\\delta_j")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.9805em","vertical-align":"-0.2861em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03785em"}},"δ"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0379em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05724em"}},"j")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])])])])]),a("，这里还是Double DQN的形式，用了两个Q网络，里面的Q网络选择动作，外面的Q网络评估这个动作")]),s("li",null,[a("用TD误差值"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"δ"),s("mi",null,"j")])]),s("annotation",{encoding:"application/x-tex"},"\\delta_j")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.9805em","vertical-align":"-0.2861em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03785em"}},"δ"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0379em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05724em"}},"j")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])])])])]),a("去更新transition的优先级"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"p"),s("mi",null,"j")])]),s("annotation",{encoding:"application/x-tex"},"p_j")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7167em","vertical-align":"-0.2861em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"p"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05724em"}},"j")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])])])])]),a("。这里是直接更新，你也可以用上面的两种去抵抗上小节说的优先级经验回放存在的一些问题（如噪声干扰等）")]),s("li",null,"实际是batch批量更新，不是每一次都更新")],-1),H=l('<h2 id="q-learning到各dqn变体的演变" tabindex="-1"><a class="header-anchor" href="#q-learning到各dqn变体的演变" aria-hidden="true">#</a> Q learning到各DQN变体的演变</h2><p>这里对上面说的各DQN方法的总结。主要是说怎么由基于表格的Q学习逐步的导出上面所说的各种DQN</p><p><img src="'+Q+'" alt="Q-learning-to-Double-DQN"></p><h2 id="rainbow" tabindex="-1"><a class="header-anchor" href="#rainbow" aria-hidden="true">#</a> Rainbow</h2><p>Rainbow就是把各种DQN变种融合到一起的算法。</p><ul><li><p>提出Rainbow的文献：Rainbow: Combining Improvements in Deep Reinforcement Learning</p></li><li><p>集合了多种 DQN 的变体</p><ul><li><p>DQN</p></li><li><p>Double DQN</p></li><li><p>Dueling DQN</p></li><li><p>Prioritized Experience Replay</p></li><li><p>NoiseNet: (Noisy Networks for Exploration, AAAI2018)</p><p>这里没讲，其实是对探索的改善。之前讲的各DQN和Q学习一样，其探索是带ε的贪婪策略，是人为设计的，人为加了一个带ε的噪声扰动。这里呢，就是说，这个扰动也可以去用神经网络学习。</p></li><li><p>Distributional RL: (A Distributional Perspective on Reinforcement Learning, ICML2017)</p><p>把强化学习通过概率的形式去解释。以前是输出每一个输出，每一个动作，每一个动作对应的Q函数，现在不输出Q函数了，而是输出Q函数的分布。从贝叶斯的观点来说，每一个东西都是随机的，都有一个分布，则对Q值，认为也是一个分布，而不是一个值</p></li></ul></li></ul><p><strong>Rainbow和其他DQN变种的对比</strong></p><p>Rainbow是上面这条线，明显比别的DQN变种都很强。</p><p><img src="'+w+'" alt="Rainbow-VS-DQNs"></p><h1 id="policy-based-drl" tabindex="-1"><a class="header-anchor" href="#policy-based-drl" aria-hidden="true">#</a> Policy-based DRL</h1><p>基于策略的深度强化学习</p><p>上节课介绍了策略梯度算法和A2C算法，今天来介绍DPG和DDPG的算法，这两个算法是自成一派，和前面的算法都不太一样。最大不同就是能用于高维和连续动作空间。</p><h2 id="dpg" tabindex="-1"><a class="header-anchor" href="#dpg" aria-hidden="true">#</a> DPG</h2><p>确定性策略梯度算法。DPG并不是一个深度强化学习算法，讲它的原因是为了引出DDPG（深度确定性策略梯度算法）。</p>',14),X=s("ul",null,[s("li",null,[s("p",null,"Deterministic Policy Gradient Algorithms (ICML2014)")]),s("li",null,[s("p",null,"将策略梯度定理用到了高维和连续动作")]),s("li",null,[s("p",null,"常规的策略梯度方法无法用到高维和连续动作空间"),s("ul",null,[s("li",null,[s("p",null,"连续动作无法使用概率分布输出的形式"),s("p",null,[a("因为常规的策略梯度（PG）算法是输出动作的概率分布，但对连续动作或者高维动作就无法计算了，比如"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mn",null,"1"),s("msup",null,[s("mn",null,"0"),s("mn",null,"100")])]),s("annotation",{encoding:"application/x-tex"},"10^{100}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8141em"}}),s("span",{class:"mord"},"1"),s("span",{class:"mord"},[s("span",{class:"mord"},"0"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"100")])])])])])])])])])])]),a("维的动作，就要构建"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mn",null,"1"),s("msup",null,[s("mn",null,"0"),s("mn",null,"100")])]),s("annotation",{encoding:"application/x-tex"},"10^{100}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8141em"}}),s("span",{class:"mord"},"1"),s("span",{class:"mord"},[s("span",{class:"mord"},"0"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"100")])])])])])])])])])])]),a("维的概率输出，显然不可行。如果是连续动作，那就是维度无穷大。")])]),s("li",null,[s("p",null,"高维动作空间中采样费时")]),s("li",null,[s("p",null,"随机策略梯度是通过采样的方式估算策略梯度, 需要在状态空间和动作空间内采样"),s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mo",null,"▽"),s("msub",null,[s("mi",null,"π"),s("mi",null,"θ")])]),s("mi",null,"J"),s("mo",{stretchy:"false"},"("),s("mi",null,"θ"),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("msub",null,[s("mo",null,"∫"),s("mi",null,"S")]),s("msub",null,[s("mo",null,"∫"),s("mi",null,"A")]),s("msup",null,[s("mi",null,"ρ"),s("msub",null,[s("mi",null,"π"),s("mi",null,"θ")])]),s("mo",{stretchy:"false"},"("),s("mi",null,"s"),s("mo",{stretchy:"false"},")"),s("msub",null,[s("mo",null,"▽"),s("mi",null,"θ")]),s("mtext",null,"log"),s("msub",null,[s("mi",null,"π"),s("mi",null,"θ")]),s("mo",{stretchy:"false"},"("),s("mi",null,"a"),s("mi",{mathvariant:"normal"},"∣"),s("mi",null,"s"),s("mo",{stretchy:"false"},")"),s("msup",null,[s("mi",null,"Q"),s("msub",null,[s("mi",null,"π"),s("mi",null,"θ")])]),s("mo",{stretchy:"false"},"("),s("mi",null,"s"),s("mo",{separator:"true"},","),s("mi",null,"a"),s("mo",{stretchy:"false"},")"),s("mtext",null," "),s("mi",null,"d"),s("mi",null,"a"),s("mo",null,"⋅"),s("mi",null,"d"),s("mi",null,"s")]),s("annotation",{encoding:"application/x-tex"}," \\bigtriangledown_{\\pi_{\\theta}}J(\\theta)=\\int_S\\int_A\\rho^{\\pi_{\\theta}}(s)\\bigtriangledown_{\\theta}\\text{log}\\pi_{\\theta}(a|s)Q^{\\pi_{\\theta}}(s,a)\\ da\\cdot ds ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0059em","vertical-align":"-0.2559em"}}),s("span",{class:"mord"},[s("span",{class:"mbin"},"▽"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"π"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3448em"}},[s("span",{style:{top:"-2.3488em","margin-left":"-0.0359em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1512em"}},[s("span")])])])])])])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2559em"}},[s("span")])])])])]),s("span",{class:"mord mathnormal",style:{"margin-right":"0.09618em"}},"J"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"θ"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"2.2719em","vertical-align":"-0.9119em"}}),s("span",{class:"mop"},[s("span",{class:"mop op-symbol large-op",style:{"margin-right":"0.44445em",position:"relative",top:"-0.0011em"}},"∫"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"-0.4336em"}},[s("span",{style:{top:"-1.7881em","margin-left":"-0.4445em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05764em"}},"S")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.9119em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mop"},[s("span",{class:"mop op-symbol large-op",style:{"margin-right":"0.44445em",position:"relative",top:"-0.0011em"}},"∫"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"-0.4336em"}},[s("span",{style:{top:"-1.7881em","margin-left":"-0.4445em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"A")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.9119em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"ρ"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7144em"}},[s("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"π"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3448em"}},[s("span",{style:{top:"-2.3488em","margin-left":"-0.0359em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1512em"}},[s("span")])])])])])])])])])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"s"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},[s("span",{class:"mbin"},"▽"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord text"},[s("span",{class:"mord"},"log")]),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"π"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"a"),s("span",{class:"mord"},"∣"),s("span",{class:"mord mathnormal"},"s"),s("span",{class:"mclose"},")"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"Q"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7144em"}},[s("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"π"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3448em"}},[s("span",{style:{top:"-2.3488em","margin-left":"-0.0359em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1512em"}},[s("span")])])])])])])])])])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"s"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"a"),s("span",{class:"mclose"},")"),s("span",{class:"mspace"}," "),s("span",{class:"mord mathnormal"},"d"),s("span",{class:"mord mathnormal"},"a"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"⋅"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6944em"}}),s("span",{class:"mord mathnormal"},"d"),s("span",{class:"mord mathnormal"},"s")])])])])])])])]),s("li",null,[s("p",null,[a("直接采用确定性策略输出："),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"a"),s("mo",null,"="),s("mi",null,"π"),s("mo",{stretchy:"false"},"("),s("mi",null,"s"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"a=\\pi(s)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"a"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"π"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"s"),s("span",{class:"mclose"},")")])])])])]),s("li",null,[s("p",null,"由此诞生几个问题："),s("ul",null,[s("li",null,"确定性策略情况下如何求策略梯度？"),s("li",null,"如何探索？")])]),s("li",null,[s("p",null,"过去一直认为无模型情况下确定性策略梯度不存在")]),s("li",null,[s("p",null,"DPG证明了确定性策略梯度定理的存在, 建立了它跟Q函数梯度的关系"),s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mo",null,"▽"),s("msub",null,[s("mi",null,"π"),s("mi",null,"θ")])]),s("mi",null,"J"),s("mo",{stretchy:"false"},"("),s("mi",null,"θ"),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("msub",null,[s("mo",null,"∫"),s("mi",null,"S")]),s("msup",null,[s("mi",null,"ρ"),s("msub",null,[s("mi",null,"π"),s("mi",null,"θ")])]),s("mo",{stretchy:"false"},"("),s("mi",null,"s"),s("mo",{stretchy:"false"},")"),s("msub",null,[s("mo",null,"▽"),s("mi",null,"θ")]),s("msub",null,[s("mi",null,"π"),s("mi",null,"θ")]),s("msub",null,[s("mo",null,"▽"),s("mi",null,"a")]),s("msup",null,[s("mi",null,"Q"),s("msub",null,[s("mi",null,"π"),s("mi",null,"θ")])]),s("mo",{stretchy:"false"},"("),s("mi",null,"s"),s("mo",{separator:"true"},","),s("mi",null,"a"),s("mo",{stretchy:"false"},")"),s("msub",null,[s("mi",{mathvariant:"normal"},"∣"),s("mrow",null,[s("mi",null,"a"),s("mo",null,"="),s("mi",null,"π"),s("mo",{stretchy:"false"},"("),s("mi",null,"θ"),s("mo",{stretchy:"false"},")")])]),s("mi",null,"d"),s("mi",null,"s")]),s("annotation",{encoding:"application/x-tex"}," \\bigtriangledown_{\\pi_{\\theta}}J(\\theta)=\\int_S\\rho^{\\pi_{\\theta}}(s)\\bigtriangledown_{\\theta}\\pi_{\\theta}\\bigtriangledown_aQ^{\\pi_{\\theta}}(s,a)|_{a=\\pi(\\theta)}ds ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0059em","vertical-align":"-0.2559em"}}),s("span",{class:"mord"},[s("span",{class:"mbin"},"▽"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"π"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3448em"}},[s("span",{style:{top:"-2.3488em","margin-left":"-0.0359em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1512em"}},[s("span")])])])])])])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2559em"}},[s("span")])])])])]),s("span",{class:"mord mathnormal",style:{"margin-right":"0.09618em"}},"J"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"θ"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"2.2719em","vertical-align":"-0.9119em"}}),s("span",{class:"mop"},[s("span",{class:"mop op-symbol large-op",style:{"margin-right":"0.44445em",position:"relative",top:"-0.0011em"}},"∫"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"-0.4336em"}},[s("span",{style:{top:"-1.7881em","margin-left":"-0.4445em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05764em"}},"S")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.9119em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"ρ"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7144em"}},[s("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"π"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3448em"}},[s("span",{style:{top:"-2.3488em","margin-left":"-0.0359em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1512em"}},[s("span")])])])])])])])])])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"s"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},[s("span",{class:"mbin"},"▽"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8889em","vertical-align":"-0.1944em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"π"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},[s("span",{class:"mbin"},"▽"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"a")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.1052em","vertical-align":"-0.3552em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"Q"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7144em"}},[s("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"π"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3448em"}},[s("span",{style:{top:"-2.3488em","margin-left":"-0.0359em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1512em"}},[s("span")])])])])])])])])])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"s"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"a"),s("span",{class:"mclose"},")"),s("span",{class:"mord"},[s("span",{class:"mord"},"∣"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3448em"}},[s("span",{style:{top:"-2.5198em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"a"),s("span",{class:"mrel mtight"},"="),s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"π"),s("span",{class:"mopen mtight"},"("),s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"θ"),s("span",{class:"mclose mtight"},")")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3552em"}},[s("span")])])])])]),s("span",{class:"mord mathnormal"},"d"),s("span",{class:"mord mathnormal"},"s")])])])])])]),s("li",null,[s("p",null,"只需要对状态S进行积分")]),s("li",null,[s("p",null,"使用Oﬀ-policy的方式探索并更新（需要重要性采样）")])],-1),Z=l('<h2 id="ddpg" tabindex="-1"><a class="header-anchor" href="#ddpg" aria-hidden="true">#</a> DDPG</h2><ul><li>Continuous Control with Deep Reinforcement Learning (ICRL2016)</li><li>结合了DQN和DPG</li><li>使用了DQN的两种技术：Experience Replay和Target Network</li><li>利用随机过程产生探索性动作</li></ul><p><img src="'+_+'" alt="PG-to-DDPG"></p><p><strong>建模方式</strong></p><p><img src="'+z+'" alt="DDPG-modeling-methods"></p><p><strong>DDPG算法</strong></p><p><img src="'+f+'" alt="DDPG-algorithm"></p><ul><li>在执行过程中有一个随机过程，用各种随机游走的方式，去建模这个噪声。用随机过程在真正的target动作下，加一个噪声去选择动作，做环境交互。</li><li>维护了target network。不仅在critic network，也在actor network也用了target network。在构建目标值的时候，用target actor选动作，用target Q去评价它的目标，这样可以使其更稳定。</li><li>target会更新的更慢一些，因为加了延迟因子τ</li></ul><h2 id="a3c" tabindex="-1"><a class="header-anchor" href="#a3c" aria-hidden="true">#</a> A3C</h2><p>A3C: Asynchronous Advantage Actor-Critic</p><p>和A2C比较相似，唯一的区别在于一个是异步更新，一个是同步更新。</p>',11),Y={href:"https://arxiv.org/abs/1602.01783",target:"_blank",rel:"noopener noreferrer"},$=s("em",null,"Asynchronous Methods for Deep Reinforcement Learning",-1),ss=s("p",null,"A3C来源于这篇文章，但这篇文章其实本质上不是讲A3C，讲的是异步更新的方法。讲了一套异步更新的方法，包括异步更新的DQN，异步更新的SARSA，异步更新的Actor Critic",-1),as=s("p",null,"这一套算法中最著名的异步更新算法就是A3C（异步更新的Actor Critic）",-1),ls=l("<li><p>Online的算法和DNN结合后不稳定 (因为有样本关联性)</p><p>DQN引入经验回放打破关联性</p><p>但我们能不能不用经验回放去做呢？用一些on-policy的方法去做。因为有了经验回放，就限制了必须用离策略形式更新，我们也想用在策略的方法，怎么办？就提出了下列方法：</p></li><li><p>通过创建多个agent在多个环境（不同进程）执行异步学习构建batch</p><ul><li><p>来自不同环境的样本无相关性</p></li><li><p>不依赖于GPU和大型分布式系统</p><p>直接利用了电脑的多进程就可以了，比较平民化的实践方式</p></li><li><p>不同线程使用了不同的探索策略，增加了探索量</p></li></ul></li>",2),ts=l('<p><img src="'+N+'" alt="PG-to-A3C"></p><p><img src="'+A+'" alt="A3C-algorithm"></p><p>每一个进程更新完，就马上把收集到的梯度推送到主进程，让主进程去更新网络。这更多的是一个工程的实现方法。A3C可以拓展到超大规模的强化学习，有好多台服务器，每台服务器开一个进程去更新，然后将参数的梯度用异步通信的方式推送到参数服务器上</p><h2 id="a2c" tabindex="-1"><a class="header-anchor" href="#a2c" aria-hidden="true">#</a> A2C</h2><ul><li><p>Openai Blog(https://blog.openai.com/baselines-acktr-a2c/)</p></li><li><p>改异步为同步, 能够更好地利用GPU</p><p>对于单机版本，就改成了同步，因为异步没什么额外好处。同步是指所有进程同步等待，变成一个batch去更新</p><p>这样的好处是可以利用GPU加速</p></li><li><p>在batch_size较大时效果好</p></li></ul><p><img src="'+k+'" alt="PG-to-A2C"></p><h2 id="策略梯度到基于策略drl的演化" tabindex="-1"><a class="header-anchor" href="#策略梯度到基于策略drl的演化" aria-hidden="true">#</a> 策略梯度到基于策略DRL的演化</h2><p><img src="'+P+'" alt="PG-to-DDPG-and-A2C"></p><h1 id="trust-region-based-drl" tabindex="-1"><a class="header-anchor" href="#trust-region-based-drl" aria-hidden="true">#</a> Trust Region based DRL</h1><p>基于深度强化学习的信赖域优化，这个其实是在基于策略的深度强化学习里面占据主要部分。主要是解决基于策略的深度强化学习中步长收敛的问题。但这里把它单独拉出来作为一条线，主要是因为现在很多好的算法和方法都是用了这样的方法，所以单独拉出来详细讲解。</p><p>信赖域的理论是相当复杂的，这里讲的只是凤毛麟角。如果你的优化理论学得不好，没有一个完备的优化理论体系，这里无论怎么讲，你都是不理解的。所以这里干脆只讲它到底做了什么事，为什么要做这个事，它到底有什么用。至于它到底怎么做的，那就要恶补很多知识才能看懂。</p><h2 id="简要介绍" tabindex="-1"><a class="header-anchor" href="#简要介绍" aria-hidden="true">#</a> 简要介绍</h2><p>之前只是推导了梯度更新的方向，但是有个关键点就是它的步长。这个非常非常关键，和值函数方法是不一样的。值函数方法本质上还是监督学习的方法，还是去构造了一些样本，输入是状态，输出是TD target，即用TD目标值的样本去更新，就是说，假设这一步迈大了，则Q值会比以前更小，就会把Q拉回来。但是策略梯度情况下，就很容易这一步迈大了之后，一更新，就把整个策略变掉了。而在Q学习中，只是把这个状态下的Q函数给变了，影响不是很大，是慢慢来的，是交替进行的，即更新一次Q，就做一次策略提升，哪怕这步没走好，影响也不是很大。但是策略梯度的问题在于，一旦把策略变掉了，后面的采样轨迹就完完全全发生了变化。</p><p>举个例子理解：走迷宫，这个迷宫相当于二叉树，一次改变，差别就很大。</p><p>策略梯度算法的更新步长很重要</p><ul><li>步长太小，导致更新效率低下</li><li>步长太大，导致参数更新的策略比上次更差，通过更差的策略采样得到的样本更差，导致学习再次更新的参数会更差，最终崩溃</li></ul><p>如何选择一个合适的步长，使得每次更新得到的新策略所实现的回报值单调不减：可限定一个信赖域，在信赖域中更新，会保证得到的新策略的表现不会比以前更差，即至少单调不减。</p><ul><li>信赖域(Trust Region)方法指在该区域内更新，策略所实现的回报值单调不减</li></ul><h2 id="一些基础" tabindex="-1"><a class="header-anchor" href="#一些基础" aria-hidden="true">#</a> 一些基础</h2><p>下面的内容其实已经超出这门课的范围了。</p>',20),ns=s("ul",null,[s("li",null,[s("p",null,"自然梯度：Natural Gradient Works Efciently in Learning, 1998"),s("ul",null,[s("li",null,[s("p",null,[a("在黎曼空间（神经网络参数空间）里面，它不是正交空间，最快的下降方向不是梯度方向，而是自然梯度方向"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"G"),s("mrow",null,[s("mo",null,"−"),s("mn",null,"1")])]),s("mo",{stretchy:"false"},"("),s("mi",null,"θ"),s("mo",{stretchy:"false"},")"),s("mi",null,"J"),s("mo",{stretchy:"false"},"("),s("mi",null,"θ"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"G^{-1}(\\theta)J(\\theta)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0641em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"G"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mtight"},"1")])])])])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"θ"),s("span",{class:"mclose"},")"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.09618em"}},"J"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"θ"),s("span",{class:"mclose"},")")])])])])]),s("li",null,[s("p",null,"只有当坐标系统正交，才退化成欧式空间")])])]),s("li",null,[s("p",null,"神经网络中的参数空间是黎曼空间")]),s("li",null,[s("p",null,"其中G为Reimannian metric tensor")]),s("li",null,[s("p",null,"统计问题中， G可以用Hessian矩阵去计算 。类似于牛顿法")]),s("li",null,[s("p",null,"保守策略迭代，CPI: Approximately Optimal Approximate Reinforcement Learning, 2002"),s("ul",null,[s("li",null,[a("给出策略性能增长的条件 "),s("ul",null,[s("li",null,"策略更新后的所有优势函数非负，就能保证新策略不会比老策略差"),s("li",null,"使用混合更新的方式更新策略")])])])])],-1),es=l('<h2 id="trpo" tabindex="-1"><a class="header-anchor" href="#trpo" aria-hidden="true">#</a> TRPO</h2><p>在信赖域优化里，一个重要的工作就是TRPO，就是结合了上节所说的自然梯度和CPI，以及信赖域方法，把三者做结合得到的，非常重要。有了这个更算法，策略梯度在很多复杂的（尤其是机器人控制）问题上才真正表现得比较好。</p><p>有了TRPO，就可以保证策略梯度的更新比较稳定，但实现起来很复杂，尤其是还要求共轭梯度（每次更新还要迭代求梯度）。</p><ul><li><p>Trust Region Policy Optimization, ICML2015</p></li><li><p>以CPI为基础，推导出策略更新后性能的下界, 通过优化<strong>下界</strong>优化原函数</p></li><li><p>实际操作时用KL散度作为约束</p><p>优化下界比较难，就把下界变成了KL散度</p></li><li><p>求解带约束的优化问题时，利用自然梯度</p></li><li><p>实际求解是利用了共轭梯度+线性搜索的方法, 避免求自然梯度</p></li></ul><p><img src="'+E+'" alt="TRPO"></p><h2 id="ppo" tabindex="-1"><a class="header-anchor" href="#ppo" aria-hidden="true">#</a> PPO</h2><p>真正在实践当中用到信赖域方法，这里推荐PPO</p><p>OpenAI做的打dota的也是用PPO实现的。自从OpenAI推出了PPO，整个OpenAI都会把它作为一个默认的算法。基本上什么算法都会用PPO去做。主要原始是PPO结合了策略梯度的一些优点，学习起来又比较稳定，执行起来也比较方便，速度也比较快，在大规模的情况下实现起来特别好。</p><ul><li><p>Proximal Policy Optimization Algorithms, 2017</p></li><li><p>Openai blog(https://blog.openai.com/openai-baselines-ppo/)</p><p>具体参考此博客</p></li><li><p>TRPO太复杂，普通策略梯度（PG）效果又不好</p></li><li><p>PPO本质上是TRPO的简化版</p></li><li><p>移除了KL惩罚项和交替更新</p><p>把TRPO的KL约束和交替更新去掉，搞成一个等价的方式，写到LOSS函数里面，加了一个正则项。则更新方式就和以前一模一样了，无非就是在Loss函数里加一项</p></li><li><p>由于性能好，且容易实现，已经成为默认的OPENAI算法</p></li></ul><p><img src="'+R+'" alt="PPO"></p><p><img src="'+M+'" alt="PPO-algorithm"></p><h2 id="其他信赖域算法" tabindex="-1"><a class="header-anchor" href="#其他信赖域算法" aria-hidden="true">#</a> 其他信赖域算法</h2><p>信赖域方法还有很多很多，下面列几个：</p><ul><li>ACKTR: Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation</li><li>ACER: Sample Efcient Actor-Critic with Experience Replay</li><li>GAE: High-Dimensional Continuous Control Using Generalized Advantage Estimation</li><li>…</li></ul><h1 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料" aria-hidden="true">#</a> 参考资料</h1>',15),is={href:"http://www.shenlanxueyuan.com/my/course/96",target:"_blank",rel:"noopener noreferrer"},ms=s("p",null,"本章内容是该课程这节课的笔记。",-1);function ps(rs,cs){const i=e("RouterLink"),n=e("ExternalLinkIcon");return p(),r("div",null,[B,s("ul",null,[s("li",null,[t(i,{to:"/docs/machine-learning/reinforcement-learning/reinforcement-learning/reinforcement-learning.html"},{default:c(()=>[a("返回上层目录")]),_:1})]),C]),T,G,O,S,I,V,j,q,J,F,K,W,U,H,X,Z,s("ul",null,[s("li",null,[s("p",null,[s("a",Y,[$,t(n)]),a(" (ICML2016)")]),ss,as]),ls]),ts,ns,es,s("ul",null,[s("li",null,[s("a",is,[a("《强化学习理论与实践》第九章-深度强化学习"),t(n)])])]),ms])}const os=m(L,[["render",ps],["__file","deep-reinforcement-learning.html.vue"]]);export{os as default};
