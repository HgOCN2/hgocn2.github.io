import{_ as o,r as p,o as c,c as l,b as n,e as s,w as i,d as a,a as r}from"./app-VitiyI7N.js";const u="/assets/bn_vs_ln-9LrGd56B.jpg",k={},d=n("h1",{id:"layer-normalization",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#layer-normalization","aria-hidden":"true"},"#"),a(" Layer Normalization")],-1),m=r('<p>对于batch normalization实际上有两种说法，一种是说BN能够解决“Internal Covariate Shift”这种问题。简单理解就是随着层数的增加，中间层的输出会发生“漂移”。另外一种说法是：BN能够解决梯度弥散。通过将输出进行适当的缩放，可以缓解梯度消失的状况。</p><p>那么NLP领域中，我们很少遇到BN，而出现了很多的LN，例如bert等模型都使用layer normalization。这是为什么呢？</p><h1 id="bn与ln主要区别" tabindex="-1"><a class="header-anchor" href="#bn与ln主要区别" aria-hidden="true">#</a> BN与LN主要区别</h1><p><strong>主要区别在于normalization的方向不同！</strong></p><p>Batch Norm顾名思义是对一个batch进行操作。假设我们有10行3列 的数据，即我们的<code>batchsize = 10</code>，每一行数据有三个特征，假设这三个特征是【身高、体重、年龄】。那么BN是针对每一列（特征）进行缩放，例如算出【身高】的均值与方差，再对身高这一列的10个数据进行缩放。体重和年龄同理。这是一种“列缩放”。</p><p>而Layer Norm方向相反，它针对的是每一行进行缩放。即只看一笔数据，算出这笔所有特征的均值与方差再缩放。这是一种“行缩放”。</p><p>细心的你已经看出来，Layer Normalization对所有的特征进行缩放，这显得很没道理。我们算出一行这【身高、体重、年龄】三个特征的均值方差并对其进行缩放，事实上会因为特征的量纲不同而产生很大的影响。但是BN则没有这个影响，因为BN是对一列进行缩放，一列的量纲单位都是相同的。</p><p>那么我们为什么还要使用LN呢？因为NLP领域中，LN更为合适。</p><p>如果我们将一批文本组成一个batch，那么BN的操作方向是，对每句话的<strong>第一个</strong>词进行操作。但语言文本的复杂性是很高的，任何一个词都有可能放在初始位置，且词序可能并不影响我们对句子的理解。而BN是<strong>针对每个位置</strong>进行缩放，这<strong>不符合NLP的规律</strong>。</p><p>而LN则是针对一句话进行缩放的，且L<strong>N一般用在第三维度</strong>，如[batchsize, seq_len, dims]中的dims，一般为词向量的维度，或者是RNN的输出维度等等，这一维度各个特征的量纲应该相同。因此也不会遇到上面因为特征的量纲不同而导致的缩放问题。</p><p>如下图所示：</p><p><img src="'+u+`" alt="bn_vs_ln"></p><p>假如我们的词向量是100维（如图是立方体的高），batchsize是64（立方体中的N）。</p><p>BN：固定每句话的第一个位置，则这个切片是 （64， 100）维的矩阵。</p><p>LN：固定一句话，则切片是（seq_len, 100）维。</p><p>但是，BN取出一条 **（1，64）<strong>的向量（<strong>绿色剪头方向</strong>）并进行缩放，LN则是取出一条</strong>（1， 100）**维（<strong>红色箭头</strong>）进行缩放。</p><h1 id="layer-normalization代码" tabindex="-1"><a class="header-anchor" href="#layer-normalization代码" aria-hidden="true">#</a> Layer-normalization代码</h1><p>Layer normalization代码实现实例如下：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">LayerNormalization</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">1e-8</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LayerNormalization<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>epsilon <span class="token operator">=</span> epsilon
    <span class="token keyword">def</span> <span class="token function">build</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">&#39;gamma&#39;</span><span class="token punctuation">,</span>
                                     shape<span class="token operator">=</span>input_shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                     initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>ones_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                     trainable<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>beta <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">&#39;beta&#39;</span><span class="token punctuation">,</span>
                                    shape<span class="token operator">=</span>input_shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                    initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>zeros_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                    trainable<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LayerNormalization<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>build<span class="token punctuation">(</span>input_shape<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># x shape=[batch_size, seq_len, d_model]</span>
        mean <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        std <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend<span class="token punctuation">.</span>std<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> <span class="token punctuation">(</span>x <span class="token operator">-</span> mean<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>std <span class="token operator">+</span> self<span class="token punctuation">.</span>epsilon<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>beta

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h1 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料" aria-hidden="true">#</a> 参考资料</h1>`,20),h={href:"https://zhuanlan.zhihu.com/p/74516930",target:"_blank",rel:"noopener noreferrer"},b=n("p",null,"本文参考了该知乎博客。",-1),_={href:"https://blog.csdn.net/qq_43079023/article/details/103301846",target:"_blank",rel:"noopener noreferrer"},v=n("p",null,"本文中的代码参考此博客。",-1);function f(N,g){const e=p("RouterLink"),t=p("ExternalLinkIcon");return c(),l("div",null,[d,n("ul",null,[n("li",null,[s(e,{to:"/docs/machine-learning/deep-learning/tips/normalization/tips.html"},{default:i(()=>[a("返回上层目录")]),_:1})])]),m,n("ul",null,[n("li",null,[n("a",h,[a("NLP中 batch normalization与 layer normalization"),s(t)])])]),b,n("ul",null,[n("li",null,[n("a",_,[a("Transformer学习总结附TF2.0代码实现"),s(t)])])]),v])}const y=o(k,[["render",f],["__file","layer-normalization.html.vue"]]);export{y as default};
