import{_ as s,r as i,o as c,c as h,b as e,e as r,w as a,d as n,a as l}from"./app-VitiyI7N.js";const d={},p=e("h1",{id:"强化学习",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#强化学习","aria-hidden":"true"},"#"),n(" 强化学习")],-1),_=e("p",null,"===",-1),f=e("h1",{id:"深度强化学习入门",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#深度强化学习入门","aria-hidden":"true"},"#"),n(" 深度强化学习入门")],-1),u={href:"https://www.zhihu.com/question/277325426/answer/2786792954",target:"_blank",rel:"noopener noreferrer"},m=e("p",null,"必须推荐王树森、黎彧君、张志华的新书《深度强化学习》，已经正式出版。这是一本正式出版前就注定成为经典的入门书籍——其在线公开课视频播放量超过一百万次，助力数万“云学生”——更加高效、方便、系统地学习相关知识。课程主页这里：https://github.com/wangshusen/DRL 还有对应的在线公开课视频和代码，B站、Github都有。下文内容来自作者王树森写的前言。",-1),g={href:"https://www.zhihu.com/question/277325426/answer/1544863580",target:"_blank",rel:"noopener noreferrer"},b=l('<p>1.看李宏毅的强化学习视频-b站随便找一个最新最全的；</p><p>2.看郭宪大佬的《深入浅出强化学习》-知乎有他的专栏文章；</p><p>3.代码刷openai的spinningup。</p><p>目前我认为最简洁最不走弯路的方法。至少节省大家半年的随机探索时间</p><p>其他的教材对于强化的公式推导不够透彻，</p><p>其他几门视频课难度高，不适合入门；</p><p>其他的代码库，新手根本看不懂。</p><p>最后贴上我基于spinup封装好的一个强化学习库：</p><p>https://github.com/kaixindelele/DRL-tensorflow</p><p>https://github.com/kaixindelele/DRLib</p><h1 id="地图" tabindex="-1"><a class="header-anchor" href="#地图" aria-hidden="true">#</a> 地图：</h1>',11),w={href:"https://mp.weixin.qq.com/s?__biz=MzU0MTgxNDkxOA%3D%3D&idx=1&mid=2247484575&scene=21&sn=42fe3fc7d5978ca9da467fde38a13245#wechat_redirect",target:"_blank",rel:"noopener noreferrer"},k={href:"https://github.com/NeuronDance/DeepRL",target:"_blank",rel:"noopener noreferrer"},L={href:"https://github.com/NeuronDance/DeepRL/tree/master/A-Guide-Resource-For-DeepRL",target:"_blank",rel:"noopener noreferrer"},x=e("h1",{id:"视频课程",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#视频课程","aria-hidden":"true"},"#"),n(" 视频课程")],-1),R={href:"https://www.bilibili.com/video/BV13W411Y75P?p=5",target:"_blank",rel:"noopener noreferrer"},D=e("p",null,"短小精悍",-1),M={href:"https://www.bilibili.com/video/BV1UE411G78S?p=2",target:"_blank",rel:"noopener noreferrer"},z={href:"https://www.bilibili.com/video/BV1UE411G78S/?p=2&vd_source=147fb813418c7610c21b6a5618c85cb7",target:"_blank",rel:"noopener noreferrer"},y={href:"https://www.bilibili.com/video/BV1MW411w79n?p=2&vd_source=147fb813418c7610c21b6a5618c85cb7",target:"_blank",rel:"noopener noreferrer"},A={href:"https://zhuanlan.zhihu.com/p/54189036",target:"_blank",rel:"noopener noreferrer"},T=e("p",null,"有空看这个，那个陈达贵的视频ppt其实就是这个。",-1),P=e("p",null,"B站上deepmind的大佬David alived的强化学习的视频，点击率甚低。看来很多国人不知道阿发狗李的研发团队的首席科学家啊。",-1),E=e("p",null,"[CS294]",-1),q=e("p",null,"初学者非常不推荐看CS294，因为真的很难，可以看David Silver的课程",-1),v=e("p",null,"[CS234]是什么？",-1),B={href:"https://www.zhihu.com/column/c_1215667894253830144",target:"_blank",rel:"noopener noreferrer"},C=e("p",null,"这个知乎专栏讲的对各种知识点的直觉理解和分析都特别好。",-1),S={href:"https://zhuanlan.zhihu.com/p/344196096",target:"_blank",rel:"noopener noreferrer"},N=e("p",null,"第一个是李宏毅老师21年最新的深度学习课程，将最新的内容都纳入了教学大纲",-1),U=e("p",null,"第二个是多智能体强化学习领域的：UCL的汪军老师新开的课程",-1),O=e("h1",{id:"仿真环境",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#仿真环境","aria-hidden":"true"},"#"),n(" 仿真环境")],-1),V={href:"https://www.zhihu.com/question/332942236/answer/1295507780",target:"_blank",rel:"noopener noreferrer"},G=e("strong",null,"Link：",-1),I={href:"https://github.com/geek-ai/MAgent",target:"_blank",rel:"noopener noreferrer"},Q=e("p",null,[n("这个是UCL汪军老师团队Mean Field 论文里用到的环境，主要研究的是当环境由"),e("strong",null,"大量智能体"),n("组成的时候的竞争和协作问题。也可以看成是复杂的Grid World环境。Render如下：")],-1),F=e("h1",{id:"强化学习与控制",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#强化学习与控制","aria-hidden":"true"},"#"),n(" 强化学习与控制")],-1),Y={href:"https://zhuanlan.zhihu.com/p/157867488",target:"_blank",rel:"noopener noreferrer"},J=e("p",null,"作者在无人机姿态控制上使用PPO训练取得了比PID更好的效果，并成功从虚拟环境迁移到了现实世界。",-1),Z={href:"https://mp.weixin.qq.com/s/bDra-n8stqJ3gcS9zr3IVA",target:"_blank",rel:"noopener noreferrer"},j=l("<p>**《强化学习与控制》**这一门课程包括11节。</p><p><strong>第1讲</strong>介绍RL概况，包括发展历史、知名学者、典型应用以及主要挑战等。</p><p><strong>第2讲</strong>介绍RL的基础知识，包括定义概念、自洽条件、最优性原理问题架构等。</p><p><strong>第3讲</strong>介绍免模型学习的蒙特卡洛法，包括Monte Carlo估计，On-policy/off-policy，重要性采样等。</p><p><strong>第4讲</strong>介绍免模型学习的时序差分法，包括它衍生的Sarsa，Q-learning，Expected Sarsa等算法。</p><p><strong>第5讲</strong>介绍带模型学习的动态规划法，包括策略迭代、值迭代、收敛性原理等。</p><p><strong>第6讲</strong>介绍间接型RL的函数近似方法，包括常用近似函数，值函数近似，策略函数近似以及所衍生的Actor-critic架构等。</p><p><strong>第7讲</strong>介绍直接型RL的策略梯度法，包括各类Policy Gradient, 以及如何从优化的观点看待RL等。</p><p><strong>第8讲</strong>介绍深度强化学习，即以神经网络为载体的RL，包括深度化典型挑战、经验性处理技巧等。</p><p><strong>第9讲</strong>介绍带模型的强化学习，即近似动态规划，包括离散时间系统的ADP，ADP与MPC的关联分析等。</p><p><strong>第10讲</strong>介绍有限时域的近似动态规划，同时介绍了状态约束的处理手段以及它与可行性之间的关系</p><p><strong>第11讲</strong>介绍RL的各类拾遗，包括POMDP、鲁棒性、多智能体、元学习、逆强化学习以及训练平台等。</p>",12),W={href:"https://mp.weixin.qq.com/s?__biz=MzU0MTgxNDkxOA%3D%3D&chksm=fb257f19cc52f60f9c5a70260fd20cc0b7974bcd300041eb7a75e83eeaaba416d2d965679ff2&idx=1&mid=2247485168&scene=21&sn=59039fc39903a4ee721712b1a2c53b77#wechat_redirect",target:"_blank",rel:"noopener noreferrer"},H=e("h1",{id:"多智能体强化学习",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#多智能体强化学习","aria-hidden":"true"},"#"),n(" 多智能体强化学习")],-1),X={href:"https://mp.weixin.qq.com/s?__biz=MzU0MTgxNDkxOA%3D%3D&chksm=fb25711ccc52f80aa10666bec175cafb2a349673f39a558811b8945392df45808a053d0f00c4&idx=1&mid=2247485685&scene=21&sn=54dcbcaf022795d05d1f8a7bf6a17c12#wechat_redirect",target:"_blank",rel:"noopener noreferrer"},K={href:"https://www.zhihu.com/question/517905386/answer/2359101768",target:"_blank",rel:"noopener noreferrer"},$=e("h1",{id:"专题",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#专题","aria-hidden":"true"},"#"),n(" 专题")],-1),ee=e("h2",{id:"transformer-rl",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#transformer-rl","aria-hidden":"true"},"#"),n(" Transformer+RL")],-1),ne={href:"https://mp.weixin.qq.com/s?__biz=Mzk0MTI1MzI0OQ==&mid=2247490100&idx=1&sn=56d484dd1cb6062b2783554b88816688&chksm=c2d46ddaf5a3e4cc96329fcbd38876a5936344df6b113e8b7f8bbe8de391a51261e76c6faf46&cur_album_id=2628430986300801025&scene=189#wechat_redirect",target:"_blank",rel:"noopener noreferrer"},re={href:"https://mp.weixin.qq.com/s?__biz=Mzk0MTI1MzI0OQ==&mid=2247490498&idx=1&sn=4b3e2174d25e530b9a388aba3331295c&chksm=c2d46c2cf5a3e53a3154792c03f4b8be1bc20700ae5a48b70e042272da46e95dcc61a425f079&cur_album_id=2628430986300801025&scene=189#wechat_redirect",target:"_blank",rel:"noopener noreferrer"},te={href:"https://mp.weixin.qq.com/s/S0PgD3SEMbrbA4OE--ZCQg",target:"_blank",rel:"noopener noreferrer"},oe=e("h1",{id:"论坛",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#论坛","aria-hidden":"true"},"#"),n(" 论坛")],-1),ae=e("p",null,"中科院自动化所2020智能决策论坛报告ppt： 论坛报告回放：https://space.bilibili.com/551888585/channel/detail?cid=167587 【柯良军】链接：https://pan.baidu.com/s/18uM3GU8HpZ2OAUIoN0timQ 提取码：rb4o 【章宗长】链接：https://pan.baidu.com/s/1hg-YPfcjCaMnUIogZXMmTQ 提取码：dhdf 【余超】链接：https://pan.baidu.com/s/1ZnU7oe8xB6YJgyVC1frY6Q 提取码：h42p 【温颖】链接：https://pan.baidu.com/s/1AhV2v_JLtiYU3gekH0d4ow 提取码：p2h7",-1),ie=e("h1",{id:"知识点",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#知识点","aria-hidden":"true"},"#"),n(" 知识点")],-1),le={href:"https://www.zhihu.com/question/57159315/answer/1855647973",target:"_blank",rel:"noopener noreferrer"},se={href:"https://www.codelast.com/%E5%8E%9F%E5%88%9B-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%87%8C%E7%9A%84-on-policy-%E5%92%8C-off-policy-%E7%9A%84%E5%8C%BA%E5%88%AB/",target:"_blank",rel:"noopener noreferrer"},ce=e("h1",{id:"框架-库",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#框架-库","aria-hidden":"true"},"#"),n(" 框架，库")],-1),he={href:"https://github.com/tensorlayer",target:"_blank",rel:"noopener noreferrer"},de=e("strong",null,"TensorLayer",-1),pe={href:"https://mp.weixin.qq.com/s?__biz=MzU0MTgxNDkxOA%3D%3D&chksm=fb25706bcc52f97dd9a496508d570dde830a1d9181bf232136993eebcad3ade1936adec6d94a&idx=1&mid=2247485826&scene=21&sn=7faa04e1a7b922d3d42059246dcadc8a#wechat_redirect",target:"_blank",rel:"noopener noreferrer"},_e=e("p",null,"一定要看，非常好",-1),fe=e("p",null,"欢迎Star：https://github.com/StepNeverStop/RLs",-1),ue=e("p",null,"本文作者使用gym,Unity3D ml-agents等环境，利用tensorflow2.0版本对29种算法进行了实现的深度强化学习训练框架，该框架具有如下特性：",-1),me=e("ul",null,[e("li",null,"实现单智能体强化学习、分层强化学习、多智能体强化学习算法等约29种"),e("li",null,"适配gym、MuJoCo、PyBullet、Unity ML-Agents等多种训练环境")],-1),ge={href:"https://github.com/mengwanglalala/RL-algorithms",target:"_blank",rel:"noopener noreferrer"},be=e("strong",null,"RL-algorithms",-1),we=e("p",null,"RL-algorithms，更新一些基础的RL代码，附带了各个算法的介绍",-1),ke={href:"https://github.com/wwxFromTju/awesome-reinforcement-learning-lib",target:"_blank",rel:"noopener noreferrer"},Le=e("p",null,"集合了各种强化学习库",-1),xe=e("p",null,[e("strong",null,"tensorlayer")],-1),Re={href:"https://zhuanlan.zhihu.com/p/72304092",target:"_blank",rel:"noopener noreferrer"},De={href:"https://www.jianshu.com/p/d206fb7a190d",target:"_blank",rel:"noopener noreferrer"},Me=e("h1",{id:"新概念",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#新概念","aria-hidden":"true"},"#"),n(" 新概念")],-1),ze=e("h2",{id:"重生强化",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#重生强化","aria-hidden":"true"},"#"),n(" 重生强化")],-1),ye=e("p",null,"我记得我刚开始学强化的时候，好奇的一个问题，对于强化的网络，如果一个开始就全给的专家数据，和从零开始学习，从试错，到自己学成专家，哪个会更好一些？ 看了Reset-RL和demo-RL之后，好像答案比较明确了，还是得有高质量的数据，然后少许交互就能快速获得一个高质量策略。而从头开始试错，不断摸索的策略，很可能会因为早期的垃圾数据，导致陷入局部最优（首因偏差)，上不去~",-1),Ae=e("p",null,"发布于 2022-12-09・IP 属地安徽",-1),Te=e("p",null,"这两个是什么算法，求指路",-1),Pe={href:"https://www.bilibili.com/video/BV1wG4y157we/?vd_source=147fb813418c7610c21b6a5618c85cb7",target:"_blank",rel:"noopener noreferrer"},Ee={href:"https://zhuanlan.zhihu.com/p/591880627",target:"_blank",rel:"noopener noreferrer"},qe=e("h1",{id:"不重要",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#不重要","aria-hidden":"true"},"#"),n(" 不重要")],-1),ve={href:"http://blog.sciencenet.cn/blog-3189881-1129931.html",target:"_blank",rel:"noopener noreferrer"};function Be(Ce,Se){const o=i("RouterLink"),t=i("ExternalLinkIcon");return c(),h("div",null,[p,e("ul",null,[e("li",null,[r(o,{to:"/docs/machine-learning/"},{default:a(()=>[n("返回上层目录")]),_:1})]),e("li",null,[r(o,{to:"/docs/machine-learning/reinforcement-learning/reinforcement-learning/reinforcement-learning.html"},{default:a(()=>[n("强化学习")]),_:1})]),e("li",null,[r(o,{to:"/docs/machine-learning/reinforcement-learning/simulation-platform/simulation-platform.html"},{default:a(()=>[n("仿真环境")]),_:1})]),e("li",null,[r(o,{to:"/docs/machine-learning/reinforcement-learning/monte-carlo-tree-search/monte-carlo-tree-search.html"},{default:a(()=>[n("MCTS蒙特卡洛树搜索")]),_:1})]),e("li",null,[r(o,{to:"/docs/machine-learning/reinforcement-learning/imatation-learning/imatation-learning.html"},{default:a(()=>[n("模仿学习")]),_:1})]),e("li",null,[r(o,{to:"/docs/machine-learning/reinforcement-learning/multi-agent-reinforcement-learning/multi-agent-reinforcement-learning.html"},{default:a(()=>[n("多智能体强化学习")]),_:1})]),e("li",null,[r(o,{to:"/docs/machine-learning/reinforcement-learning/transformer-rl/transformer-rl.html"},{default:a(()=>[n("Transformer+RL")]),_:1})]),e("li",null,[r(o,{to:"/docs/machine-learning/reinforcement-learning/decision-making-big-model/decision-making-big-model.html"},{default:a(()=>[n("决策大模型")]),_:1})]),e("li",null,[r(o,{to:"/docs/machine-learning/reinforcement-learning/offline-reinforcement-learning/offline-reinforcement-learning.html"},{default:a(()=>[n("Offline RL离线强化学习")]),_:1})]),e("li",null,[r(o,{to:"/docs/machine-learning/reinforcement-learning/multi-modal-reinforcement-learning/multi-modal-reinforcement-learning.html"},{default:a(()=>[n("MMRL多模态强化学习")]),_:1})]),e("li",null,[r(o,{to:"/docs/machine-learning/reinforcement-learning/llm-rl/llm-rl.html"},{default:a(()=>[n("LLM+RL")]),_:1})]),e("li",null,[r(o,{to:"/docs/machine-learning/reinforcement-learning/diffusion-model-rl/diffusion-model-rl.html"},{default:a(()=>[n("DiffusionModel+RL")]),_:1})]),e("li",null,[r(o,{to:"/docs/machine-learning/reinforcement-learning/industry-application/industry-application.html"},{default:a(()=>[n("业界应用")]),_:1})])]),_,f,e("p",null,[e("a",u,[n("强化学习怎么入门好？"),r(t)])]),m,e("p",null,[e("a",g,[n("强化学习怎么入门好？"),r(t)])]),b,e("p",null,[e("a",w,[n("全网首发|| 最全深度强化学习资料(永久更新)"),r(t)])]),e("p",null,[e("a",k,[n("NeuronDance/DeepRL"),r(t)])]),e("p",null,[e("a",L,[n("NeuronDance/DeepRL/A-Guide-Resource-For-DeepRL/"),r(t)])]),x,e("p",null,[e("a",R,[n("【莫烦Python】强化学习 Reinforcement Learning"),r(t)])]),D,e("p",null,[e("a",M,[n("李宏毅】2020 最新课程 (完整版) 强化学习 "),r(t)])]),e("p",null,[n("看这个，讲的很好很清楚，比如其中强化学习"),e("a",z,[n("策略梯度"),r(t)]),n("的部分。")]),e("p",null,[e("a",y,[n("李宏毅深度强化学习(国语)课程(2018) ppo"),r(t)])]),e("p",null,[e("a",A,[n("David Silver 增强学习——Lecture 6 值函数逼近"),r(t)])]),T,P,E,q,v,e("p",null,[e("a",B,[n("白话强化学习"),r(t)])]),C,e("p",null,[e("a",S,[n("强化学习路线推荐及资料整理"),r(t)])]),N,U,O,e("p",null,[e("a",V,[n("有哪些常用的多智能体强化学习仿真环境？"),r(t)])]),e("p",null,[G,e("a",I,[n("https://github.com/geek-ai/MAgent"),r(t)])]),Q,F,e("p",null,[e("a",Y,[n("强化学习无人机交互环境汇总"),r(t)])]),J,e("p",null,[e("a",Z,[n("【重磅推荐: 强化学习课程】清华大学李升波老师《强化学习与控制》"),r(t)])]),j,e("p",null,[e("a",W,[n("强化学习和最优控制的《十个关键点》81页PPT汇总"),r(t)])]),H,e("p",null,[e("a",X,[n("【DeepMind】多智能体学习231页PPT总结"),r(t)])]),e("p",null,[e("a",K,[n("最近在写多智能体强化学习工作绪论，请问除了 MADDPG 以及 MAPPO 还有哪些算法？"),r(t)])]),$,ee,e("p",null,[e("a",ne,[n("Transformer + RL专题 | 究竟是强化学习魔高一尺，还是Transformer道高一丈 （第1期）"),r(t)])]),e("p",null,[e("a",re,[n("Transformer + RL专题｜强化学习中时序建模的千层套路（第2期）"),r(t)])]),e("p",null,[e("a",te,[n("Transformer + RL 专题｜大力出奇迹，看 Transformer 如何征服超参数搜索中的决策问题 （第3期）"),r(t)])]),oe,ae,ie,e("p",null,[e("a",le,[n("强化学习中on-policy 与off-policy有什么区别？"),r(t)])]),e("p",null,[e("a",se,[n("[原创] 强化学习里的 on-policy 和 off-policy 的区别"),r(t)])]),ce,e("p",null,[n("["),e("a",he,[n("tensorlayer"),r(t)]),n("/"),de,n("](https://github.com/tensorlayer/TensorLayer/blob/cb4eb896dd063e650ef22533ed6fa6056a71cad5/examples/reinforcement_learning/README.md)")]),e("p",null,[e("a",pe,[n("Tensorflow2.0实现29种深度强化学习算法大汇总"),r(t)])]),_e,fe,ue,me,e("p",null,[e("a",ge,[n("mengwanglalala/"),be,r(t)])]),we,e("p",null,[e("a",ke,[n("Awesome Reinforcement Learning Library"),r(t)])]),Le,xe,e("p",null,[e("a",Re,[n("对话TensorLayer项目发起者董豪"),r(t)])]),e("p",null,[e("a",De,[n("TensorLayer进阶资源"),r(t)])]),Me,ze,ye,Ae,Te,e("p",null,[n("后者是DDPGfD，前者是primacy bias in rl "),e("a",Pe,[n("ResetNet-The Primacy Bias in Deep Reinforcement Learning"),r(t)])]),e("p",null,[e("a",Ee,[n("重生强化【Reincarnating RL】论文梳理"),r(t)])]),qe,e("p",null,[e("a",ve,[n("【RL系列】强化学习基础知识汇总"),r(t)])])])}const Ue=s(d,[["render",Be],["__file","reinforcement-learning.html.vue"]]);export{Ue as default};
